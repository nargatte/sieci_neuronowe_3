{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__with_3_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__with_3_neighbors__one_prediction__without_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__one_prediction__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H1 - addition of 3 closest cities will increase accuracy with similar convergence, but with longer compute time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h1_net_1(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_2(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_3(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "    \n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_4(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 1500, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 1000, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 600, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "Regression without neighbors:\n",
    "1. 4 epochs, 33.42% / 8.11%, 12.62 s\n",
    "2. 41 epochs, 45.92% / 8.54%, 187.69 s\n",
    "\n",
    "Regression with neighbors:\n",
    "1. 5 epochs, 14.95% / 10.48%, 40.98 s\n",
    "2. 4 epochs, 48.76% / 6.02%, 64.36 s\n",
    "\n",
    "Classification without neighbors:\n",
    "1. 10 epochs, 75.52% / 60.77 %, 0.5 / 0.5, 44.42 s\n",
    "2. 11 epochs, 75.52% / 60.77 %, 0.5 / 0.5, 66.58 s\n",
    "\n",
    "Classification with neighbors:\n",
    "1. 12 epochs, 76.12% / 60.82 %, 0.5 / 0.5, 77.07 s (not better, just less cases after nan filtering)\n",
    "2. 15 epochs, 76.12% / 60.82 %, 0.5 / 0.5, 267.18 s (not better, just less cases after nan filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output == \"output_temp\")\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, output)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    if output == \"output_temp\":\n",
    "        predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "        expected = denormalized(train_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "        predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "        expected = denormalized(test_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "    else:\n",
    "        predicted = net.predict(train_set)[0, :]\n",
    "        expected = train_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "        predicted = net.predict(test_set)[0, :]\n",
    "        expected = test_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h1\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h1_net_1, \"output_temp\", train_set1, test_set1, params1, \"n1_r_no_neighbors.png\")\n",
    "test(get_h1_net_2, \"output_temp\", train_set1, test_set1, params1, \"n2_r_no_neighbors.png\")\n",
    "test(get_h1_net_3, \"output_temp\", train_set2, test_set2, params2, \"n3_r_neighbors.png\")\n",
    "test(get_h1_net_4, \"output_temp\", train_set2, test_set2, params2, \"n4_r_neighbors.png\")\n",
    "\n",
    "test(get_h1_net_1, \"output_wind\", train_set1, test_set1, params1, \"n1_c_no_neighbors.png\")\n",
    "test(get_h1_net_2, \"output_wind\", train_set1, test_set1, params1, \"n2_c_no_neighbors.png\")\n",
    "test(get_h1_net_3, \"output_wind\", train_set2, test_set2, params2, \"n3_c_neighbors.png\")\n",
    "test(get_h1_net_4, \"output_wind\", train_set2, test_set2, params2, \"n4_c_neighbors.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2 - mean from 24 predictions will be better for architectures with little number of weights, whereas 1 prediction will be better when many weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h2_net_1(output_size):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, output_size, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h2_net_2(output_size):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, output_size, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "1 prediction:\n",
    "1. \n",
    "2. \n",
    "\n",
    "24 predictions:\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output_size, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output_size)\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = np.mean(denormalized(net.predict(train_set), params[\"temperature\"]), axis=0)\n",
    "    expected = np.mean(denormalized(train_set[\"output_temp\"], params[\"temperature\"]), axis=0)\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = np.mean(denormalized(net.predict(test_set), params[\"temperature\"]), axis=0)\n",
    "    expected = np.mean(denormalized(test_set[\"output_temp\"], params[\"temperature\"]), axis=0)\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h2\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h2_net_1, 1, train_set1, test_set1, params1, \"n1_r_1_prediction.png\")\n",
    "test(get_h2_net_2, 1, train_set1, test_set1, params1, \"n2_r_1_prediction.png\")\n",
    "test(get_h2_net_1, 24, train_set3, test_set3, params3, \"n1_r_24_predictions.png\")\n",
    "test(get_h2_net_2, 24, train_set3, test_set3, params3, \"n2_r_24_predictions.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H3 - L2 grants faster convergence than L1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_net_1(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_2(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_3(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_4(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.sigmoid, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.sigmoid, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_5(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "L1:\n",
    "1. 26 epochs, 55.16% / 11.34%, 302.85 s\n",
    "2. 6 epochs, 15.73% / 15.23%, 94.07 s\n",
    "3. 19 epochs, 44.79% / 7.32%, 132.44 s\n",
    "4. 6 epochs, 42.57% /6.37%, 52.23 s\n",
    "\n",
    "L2:\n",
    "1. 6 epochs, 52.48% / 7.49%, 66.24 s\n",
    "2. 39 epochs, 15.28% / 15.01%, 633.67 s\n",
    "3. 4 epochs, 28.33% / 7.64%, 23.25%\n",
    "4. 8 epochs, 38.05% / 8.22%, 65.50 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, loss, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(loss)\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h3\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h3_net_1, d.l1_loss, train_set1, test_set1, params1, \"n1_l1.png\")\n",
    "test(get_h3_net_2, d.l1_loss, train_set1, test_set1, params1, \"n2_l1.png\")\n",
    "test(get_h3_net_3, d.l1_loss, train_set1, test_set1, params1, \"n3_l1.png\")\n",
    "test(get_h3_net_4, d.l1_loss, train_set1, test_set1, params1, \"n4_l1.png\")\n",
    "test(get_h3_net_5, d.l1_loss, train_set1, test_set1, params1, \"n5_l1.png\")\n",
    "\n",
    "test(get_h3_net_1, d.l2_loss, train_set1, test_set1, params1, \"n1_l2.png\")\n",
    "test(get_h3_net_2, d.l2_loss, train_set1, test_set1, params1, \"n2_l2.png\")\n",
    "test(get_h3_net_3, d.l2_loss, train_set1, test_set1, params1, \"n3_l2.png\")\n",
    "test(get_h3_net_4, d.l2_loss, train_set1, test_set1, params1, \"n4_l2.png\")\n",
    "test(get_h3_net_5, d.l2_loss, train_set1, test_set1, params1, \"n5_l2.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## H4 - cross-entropy and hinge will not differ in terms of convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h4_net_1(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.relu, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_2(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_3(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.relu, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_4(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.sigmoid, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.sigmoid, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, loss, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(loss)\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, \"output_wind\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "    \n",
    "    predicted = net.predict(train_set)[0, :]\n",
    "    expected = train_set[\"output_wind\"][0, :]\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    auc.update_state(expected, predicted)\n",
    "    predicted = np.rint(predicted)\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "    print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    predicted = net.predict(test_set)[0, :]\n",
    "    expected = test_set[\"output_wind\"][0, :]\n",
    "    auc = tf.keras.metrics.AUC()\n",
    "    auc.update_state(expected, predicted)\n",
    "    predicted = np.rint(predicted)\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "    print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h4\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(get_h4_net_1, d.cross_entropy_loss, train_set1, test_set1, params1, \"n1_cross_entropy.png\")\n",
    "# test(get_h4_net_2, d.cross_entropy_loss, train_set1, test_set1, params1, \"n2_cross_entropy.png\")\n",
    "# test(get_h4_net_3, d.cross_entropy_loss, train_set1, test_set1, params1, \"n3_cross_entropy.png\")\n",
    "test(get_h4_net_4, d.cross_entropy_loss, train_set1, test_set1, params1, \"n4_cross_entropy.png\")\n",
    "\n",
    "# test(get_h4_net_1, d.hinge_loss, train_set1, test_set1, params1, \"n1_hinge.png\")\n",
    "# test(get_h4_net_2, d.hinge_loss, train_set1, test_set1, params1, \"n2_hinge.png\")\n",
    "# test(get_h4_net_3, d.hinge_loss, train_set1, test_set1, params1, \"n3_hinge.png\")\n",
    "# test(get_h4_net_4, d.hinge_loss, train_set1, test_set1, params1, \"n4_hinge.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 - the more shared weights in network, the smaller difference between accuracy on train and test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h5_net_1(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "        \n",
    "\n",
    "def get_h5_net_2(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8, d1)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8, d1)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h5_net_3(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8, d1)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8, d1)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.7)\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d1_n1 = nn.FullConnectLayer(d1_n1, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d2_n1 = nn.FullConnectLayer(d2_n1, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    d3_n1 = nn.FullConnectLayer(d3_n1, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "    days_n1 = nn.FullConnectLayer(days_n1, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d1_n2 = nn.FullConnectLayer(d1_n2, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d2_n2 = nn.FullConnectLayer(d2_n2, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    d3_n2 = nn.FullConnectLayer(d3_n2, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "    days_n2 = nn.FullConnectLayer(days_n2, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d1_n3 = nn.FullConnectLayer(d1_n3, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d2_n3 = nn.FullConnectLayer(d2_n3, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    d3_n3 = nn.FullConnectLayer(d3_n3, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "    days_n3 = nn.FullConnectLayer(days_n3, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([coords, city_one_hot])\n",
    "    city = nn.FullConnectLayer(city, 38, d.relu, rng, 0.8)\n",
    "\n",
    "    coords_n1 = nn.InputLayer(2, \"coords_n1\")\n",
    "    city_one_hot_n1 = nn.InputLayer(36, \"city_one_hot_n1\")\n",
    "    city_n1 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n1 = nn.FullConnectLayer(city_n1, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    coords_n2 = nn.InputLayer(2, \"coords_n2\")\n",
    "    city_one_hot_n2 = nn.InputLayer(36, \"city_one_hot_n2\")\n",
    "    city_n2 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n2 = nn.FullConnectLayer(city_n2, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    coords_n3 = nn.InputLayer(2, \"coords_n3\")\n",
    "    city_one_hot_n3 = nn.InputLayer(36, \"city_one_hot_n3\")\n",
    "    city_n3 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n3 = nn.FullConnectLayer(city_n3, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    cities = nn.MergeLayer([city, city_n1, city_n2, city_n3])\n",
    "    \n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "\n",
    "    output = nn.MergeLayer([days, cities, date])\n",
    "    output = nn.FullConnectLayer(output, 1500, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 1000, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 600, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output == \"output_temp\")\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, output)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    if output == \"output_temp\":\n",
    "        predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "        expected = denormalized(train_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "        predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "        expected = denormalized(test_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "    else:\n",
    "        predicted = net.predict(train_set)[0, :]\n",
    "        expected = train_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "        predicted = net.predict(test_set)[0, :]\n",
    "        expected = test_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    display(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h5_net_11, \"output_temp\", train_set1, test_set1, params1, \"n1_r.png\")\n",
    "# test(get_h5_net_2, \"output_temp\", train_set1, test_set1, params1, \"n2_r.png\")\n",
    "# test(get_h5_net_3, \"output_temp\", train_set2, test_set2, params2, \"n3_r.png\")\n",
    "\n",
    "# test(get_h5_net_1, \"output_wind\", train_set1, test_set1, params1, \"n1_c.png\")\n",
    "# test(get_h5_net_2, \"output_wind\", train_set1, test_set1, params1, \"n2_c.png\")\n",
    "# test(get_h5_net_3, \"output_wind\", train_set2, test_set2, params2, \"n3_c.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H6 - classification will have better accuracy using ReLUs, while regression will have better accuracy using sigmoids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h6_net_1(activation):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, activation, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, activation, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, activation, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h6_net_2(activation):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, activation, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, activation, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, activation, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, activation, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.hinge_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output, activation, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(activation)\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, output)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    if output == \"output_temp\":\n",
    "        predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "        expected = denormalized(train_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "        predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "        expected = denormalized(test_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "    else:\n",
    "        predicted = net.predict(train_set)[0, :]\n",
    "        expected = train_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "        predicted = net.predict(test_set)[0, :]\n",
    "        expected = test_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h6\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h6_net_1, \"output_temp\", d.relu, train_set1, test_set1, params1, \"n1_r_relu.png\")\n",
    "test(get_h6_net_1, \"output_temp\", d.sigmoid, train_set1, test_set1, params1, \"n1_r_sigmoid.png\")\n",
    "test(get_h6_net_2, \"output_wind\", d.relu, train_set1, test_set1, params1, \"n2_c_relu.png\")\n",
    "test(get_h6_net_2, \"output_wind\", d.sigmoid, train_set1, test_set1, params1, \"n2_c_sigmoid.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H7 - aggregation will grant similar results with less computations needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h7_net_1(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h7_net_2(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(40, \"d1\")\n",
    "    d2 = nn.InputLayer(40, \"d2\")\n",
    "    d3 = nn.InputLayer(40, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "\n",
    "def get_h7_net_3(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(40, \"d1\")\n",
    "    d2 = nn.InputLayer(40, \"d2\")\n",
    "    d3 = nn.InputLayer(40, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 120, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 100, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output == \"output_temp\")\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, output)\n",
    "    end = time.time()\n",
    "\n",
    "    if output == \"output_temp\":\n",
    "        print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "        predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "        expected = denormalized(train_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "        predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "        expected = denormalized(test_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "    else:\n",
    "        predicted = net.predict(train_set)[0, :]\n",
    "        expected = train_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "        predicted = net.predict(test_set)[0, :]\n",
    "        expected = test_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h7\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h7_net_1, \"output_temp\", train_set1, test_set1, params1, \"n1_r.png\")\n",
    "test(get_h7_net_2, \"output_temp\", train_set4, test_set4, params4, \"n2_r.png\")\n",
    "test(get_h7_net_3, \"output_temp\", train_set4, test_set4, params4, \"n3_r.png\")\n",
    "\n",
    "test(get_h7_net_1, \"output_wind\", train_set1, test_set1, params1, \"n1_c.png\")\n",
    "test(get_h7_net_2, \"output_wind\", train_set4, test_set4, params4, \"n2_c.png\")\n",
    "test(get_h7_net_3, \"output_wind\", train_set4, test_set4, params4, \"n3_c.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H8 - predicting middle day will grant better accuracy, but worse convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h8_net_1(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)\n",
    "\n",
    "def get_h8_net_2(is_regression):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    middle = nn.MergeLayer([days, city])\n",
    "    middle = nn.FullConnectLayer(middle, 400, d.relu, rng, 0.8)\n",
    "    middle = nn.FullConnectLayer(middle, 250, d.relu, rng, 0.6)\n",
    "    middle = nn.FullConnectLayer(middle, 80, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        middle = nn.FullConnectLayer(middle, 1, d.linear, rng, 1)\n",
    "    else:\n",
    "        middle = nn.FullConnectLayer(middle, 2, d.sigmoid, rng, 1)\n",
    "        middle = nn.FullConnectLayer(middle, 2, d.softmax, rng, 1)\n",
    "\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d2, d3, middle])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "\n",
    "    if is_regression:\n",
    "        output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "    else:\n",
    "        output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "        output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "        return nn.NeuralNetwork(output, d.hinge_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output, train_set, test_set, params, figname):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output == \"output_temp\")\n",
    "\n",
    "    start = time.time()\n",
    "    (train_losses, test_losses) = net.train(train_set, test_set, 1024, output)\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    if output == \"output_temp\":\n",
    "        predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "        expected = denormalized(train_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "        predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "        expected = denormalized(test_set[output], params[\"temperature\"])\n",
    "        diffs = np.abs(predicted - expected)\n",
    "        print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "    else:\n",
    "        predicted = net.predict(train_set)[0, :]\n",
    "        expected = train_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[train] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "        predicted = net.predict(test_set)[0, :]\n",
    "        expected = test_set[\"output_wind\"][0, :]\n",
    "        auc = tf.keras.metrics.AUC()\n",
    "        auc.update_state(expected, predicted)\n",
    "        predicted = np.rint(predicted)\n",
    "        print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "        print(f\"[test] AUC: {auc.result().numpy()}\")\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = fig.gca()\n",
    "    ax.plot(range(1, len(train_losses) + 1), train_losses, \"-o\")\n",
    "    ax.plot(range(1, len(test_losses) + 1), test_losses, \"-o\")\n",
    "    ax.xaxis.set_label(\"epoch\")\n",
    "    ax.xaxis.get_major_locator().set_params(integer=True)\n",
    "    ax.yaxis.set_label(\"loss\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.legend([\"train\", \"test\"])\n",
    "    fig.savefig(os.path.join(\"plots\", \"h8\", figname))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h8_net_1, \"output_temp\", train_set1, test_set1, params1, \"n1_r_no_mid_prediction.png\")\n",
    "test(get_h8_net_2, \"output_temp\", train_set1, test_set1, params1, \"n2_r_mid_prediction.png\")\n",
    "\n",
    "test(get_h8_net_1, \"output_wind\", train_set1, test_set1, params1, \"n1_c_no_mid_prediction.png\")\n",
    "test(get_h8_net_2, \"output_wind\", train_set1, test_set1, params1, \"n2_c_mid_prediction.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
