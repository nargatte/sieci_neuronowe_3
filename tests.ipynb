{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__with_3_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__with_3_neighbors__one_prediction__without_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__one_prediction__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H1 - addition of 3 closest cities will increase accuracy with similar convergence, but with longer compute time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h1_net_1():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_2():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_3():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h1_net_4():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 1500, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 1000, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 600, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function()\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 0.01551617223987023, max: 71.08680717130161, mean: 26.250113871994717, median: 26.82465848711172\n",
      "[train] Good predictions: 514, bad predictions: 45169, success rate:  1.13%\n",
      "[test] min: 0.0039347999901906405, max: 63.12207785925918, mean: 27.059985772130794, median: 28.0215699226182\n",
      "[test] Good predictions: 153, bad predictions: 12258, success rate:  1.23%\n",
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 25.240318743408864, max: 101.33386863742663, mean: 71.20910057418463, median: 72.78123556401633\n",
      "[train] Good predictions: 0, bad predictions: 45683, success rate:  0.00%\n",
      "[test] min: 33.6707565564503, max: 102.2251618102656, mean: 78.0374706669843, median: 79.20339084122168\n",
      "[test] Good predictions: 0, bad predictions: 12411, success rate:  0.00%\n",
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 21.917327958772205, max: 99.79323443787868, mean: 69.92191050177784, median: 71.38286246408329\n",
      "[train] Good predictions: 0, bad predictions: 39106, success rate:  0.00%\n",
      "[test] min: 27.455355043517187, max: 99.84136686951541, mean: 70.71934930671388, median: 71.61979500425151\n",
      "[test] Good predictions: 0, bad predictions: 11530, success rate:  0.00%\n",
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 7.8601009693353205, max: 76.64128126882682, mean: 51.594484970258975, median: 52.8341135583169\n",
      "[train] Good predictions: 0, bad predictions: 39106, success rate:  0.00%\n",
      "[test] min: 12.69708241632776, max: 80.55613080015809, mean: 50.31879202213323, median: 51.293067166377725\n",
      "[test] Good predictions: 0, bad predictions: 11530, success rate:  0.00%\n"
     ]
    }
   ],
   "source": [
    "test(get_h1_net_1, train_set1, test_set1, params1)\n",
    "test(get_h1_net_2, train_set1, test_set1, params1)\n",
    "test(get_h1_net_3, train_set2, test_set2, params2)\n",
    "test(get_h1_net_4, train_set2, test_set2, params2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H2 - mean from 24 predictions will be better for architectures with little number of weights, whereas 1 prediction will be better when many weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h2_net_1(output_size):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, output_size, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h2_net_2(output_size):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 300, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 160, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, output_size, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "1 prediction:\n",
    "1. \n",
    "2. \n",
    "\n",
    "24 predictions:\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, output_size, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(output_size)\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = np.mean(denormalized(net.predict(train_set), params[\"temperature\"]), axis=0)\n",
    "    expected = np.mean(denormalized(train_set[\"output_temp\"], params[\"temperature\"]), axis=0)\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = np.mean(denormalized(net.predict(test_set), params[\"temperature\"]), axis=0)\n",
    "    expected = np.mean(denormalized(test_set[\"output_temp\"], params[\"temperature\"]), axis=0)\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h2_net_1, 1, train_set1, test_set1, params1)\n",
    "test(get_h2_net_2, 1, train_set1, test_set1, params1)\n",
    "test(get_h2_net_1, 24, train_set3, test_set3, params3)\n",
    "test(get_h2_net_2, 24, train_set3, test_set3, params3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H3 - L2 grants faster convergence than L1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h3_net_1(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_2(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_3(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h3_net_4(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.sigmoid, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.sigmoid, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results\n",
    "\n",
    "L1:\n",
    "1. 26 epochs, 55.16% / 11.34%, 302.85 s\n",
    "2. 6 epochs, 15.73% / 15.23%, 94.07 s\n",
    "3. 19 epochs, 44.79% / 7.32%, 132.44 s\n",
    "4. 6 epochs, 42.57% /6.37%, 52.23 s\n",
    "\n",
    "L2:\n",
    "1. 6 epochs, 52.48% / 7.49%, 66.24 s\n",
    "2. 39 epochs, 15.28% / 15.01%, 633.67 s\n",
    "3. 4 epochs, 28.33% / 7.64%, 23.25%\n",
    "4. 8 epochs, 38.05% / 8.22%, 65.50 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, loss, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(loss)\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test(get_h3_net_1, d.l1_loss, train_set1, test_set1, params1)\n",
    "# test(get_h3_net_2, d.l1_loss, train_set1, test_set1, params1)\n",
    "# test(get_h3_net_3, d.l1_loss, train_set1, test_set1, params1)\n",
    "# test(get_h3_net_4, d.l1_loss, train_set1, test_set1, params1)\n",
    "\n",
    "# test(get_h3_net_1, d.l2_loss, train_set1, test_set1, params1)\n",
    "# test(get_h3_net_2, d.l2_loss, train_set1, test_set1, params1)\n",
    "# test(get_h3_net_3, d.l2_loss, train_set1, test_set1, params1)\n",
    "test(get_h3_net_4, d.l2_loss, train_set1, test_set1, params1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## H4 - cross-entropy and hinge will not differ in terms of convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h4_net_1(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.relu, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_2(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.sigmoid, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_3(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.relu, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)\n",
    "\n",
    "def get_h4_net_4(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.sigmoid, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.sigmoid, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.sigmoid, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.sigmoid, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.sigmoid, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 2, d.sigmoid, rng, 1)\n",
    "    output = nn.FullConnectLayer(output, 2, d.softmax, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, loss, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function(loss)\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h4_net_1, d.cross_entropy_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_2, d.cross_entropy_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_3, d.cross_entropy_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_4, d.cross_entropy_loss, train_set1, test_set1, params1)\n",
    "\n",
    "test(get_h4_net_1, d.hinge_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_2, d.hinge_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_3, d.hinge_loss, train_set1, test_set1, params1)\n",
    "test(get_h4_net_4, d.hinge_loss, train_set1, test_set1, params1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H5 - the more shared weights in network, the smaller difference between accuracy on train and test set"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h5_net_1():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h5_net_2():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8, d1)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8, d1)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h5_net_3():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d1 = nn.FullConnectLayer(d1, 120, d.relu, rng, 0.8)\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d2 = nn.FullConnectLayer(d2, 120, d.relu, rng, 0.8, d1)\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    d3 = nn.FullConnectLayer(d3, 120, d.relu, rng, 0.8, d1)\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.7)\n",
    "\n",
    "    d1_n1 = nn.InputLayer(120, \"d1_n1\")\n",
    "    d1_n1 = nn.FullConnectLayer(d1_n1, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n1 = nn.InputLayer(120, \"d2_n1\")\n",
    "    d2_n1 = nn.FullConnectLayer(d2_n1, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n1 = nn.InputLayer(120, \"d3_n1\")\n",
    "    d3_n1 = nn.FullConnectLayer(d3_n1, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n1 = nn.MergeLayer([d1_n1, d2_n1, d3_n1])\n",
    "    days_n1 = nn.FullConnectLayer(days_n1, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    d1_n2 = nn.InputLayer(120, \"d1_n2\")\n",
    "    d1_n2 = nn.FullConnectLayer(d1_n2, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n2 = nn.InputLayer(120, \"d2_n2\")\n",
    "    d2_n2 = nn.FullConnectLayer(d2_n2, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n2 = nn.InputLayer(120, \"d3_n2\")\n",
    "    d3_n2 = nn.FullConnectLayer(d3_n2, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n2 = nn.MergeLayer([d1_n2, d2_n2, d3_n2])\n",
    "    days_n2 = nn.FullConnectLayer(days_n2, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    d1_n3 = nn.InputLayer(120, \"d1_n3\")\n",
    "    d1_n3 = nn.FullConnectLayer(d1_n3, 120, d.relu, rng, 0.8, d1)\n",
    "    d2_n3 = nn.InputLayer(120, \"d2_n3\")\n",
    "    d2_n3 = nn.FullConnectLayer(d2_n3, 120, d.relu, rng, 0.8, d2)\n",
    "    d3_n3 = nn.InputLayer(120, \"d3_n3\")\n",
    "    d3_n3 = nn.FullConnectLayer(d3_n3, 120, d.relu, rng, 0.8, d3)\n",
    "    days_n3 = nn.MergeLayer([d1_n3, d2_n3, d3_n3])\n",
    "    days_n3 = nn.FullConnectLayer(days_n3, 360, d.relu, rng, 0.7, days)\n",
    "\n",
    "    days = nn.MergeLayer([days, days_n1, days_n2, days_n3])\n",
    "\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([coords, city_one_hot])\n",
    "    city = nn.FullConnectLayer(city, 38, d.relu, rng, 0.8)\n",
    "\n",
    "    coords_n1 = nn.InputLayer(2, \"coords_n1\")\n",
    "    city_one_hot_n1 = nn.InputLayer(36, \"city_one_hot_n1\")\n",
    "    city_n1 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n1 = nn.FullConnectLayer(city_n1, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    coords_n2 = nn.InputLayer(2, \"coords_n2\")\n",
    "    city_one_hot_n2 = nn.InputLayer(36, \"city_one_hot_n2\")\n",
    "    city_n2 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n2 = nn.FullConnectLayer(city_n2, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    coords_n3 = nn.InputLayer(2, \"coords_n3\")\n",
    "    city_one_hot_n3 = nn.InputLayer(36, \"city_one_hot_n3\")\n",
    "    city_n3 = nn.MergeLayer([coords, city_one_hot])\n",
    "    city_n3 = nn.FullConnectLayer(city_n3, 38, d.relu, rng, 0.8, city)\n",
    "\n",
    "    cities = nn.MergeLayer([city, city_n1, city_n2, city_n3])\n",
    "    \n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "\n",
    "    output = nn.MergeLayer([days, cities, date])\n",
    "    output = nn.FullConnectLayer(output, 1500, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 1000, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 600, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 50, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function()\n",
    "\n",
    "    start = time.time()\n",
    "    # net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 24.04787014057075, max: 93.96597068277427, mean: 65.47355198777538, median: 66.73569687932687\n",
      "[train] Good predictions: 0, bad predictions: 45683, success rate:  0.00%\n",
      "[test] min: 23.774509221220114, max: 89.44020466536219, mean: 64.91547137396374, median: 66.25723278804026\n",
      "[test] Good predictions: 0, bad predictions: 12411, success rate:  0.00%\n",
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n",
      "[train] min: 9.975554843898749, max: 79.97972542961682, mean: 53.67048761016801, median: 54.94600622241154\n",
      "[train] Good predictions: 0, bad predictions: 45683, success rate:  0.00%\n",
      "[test] min: 11.633948576550353, max: 74.69790989619102, mean: 53.55534263504296, median: 54.68058427922551\n",
      "[test] Good predictions: 0, bad predictions: 12411, success rate:  0.00%\n",
      "================================== new test ==================================\n",
      "Time elapsed:  0.00 s\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "test(get_h5_net_1, train_set1, test_set1, params1)\n",
    "test(get_h5_net_2, train_set1, test_set1, params1)\n",
    "test(get_h5_net_3, train_set2, test_set2, params2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H6 - classification will have better accuracy using ReLUs, while regression will have better accuracy using sigmoids"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H7 - aggregation will grant similar results with less computations needed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h7_net_1():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 499, d.relu, rng, 0.7)\n",
    "    output = nn.FullConnectLayer(output, 350, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h7_net_2():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(40, \"d1\")\n",
    "    d2 = nn.InputLayer(40, \"d2\")\n",
    "    d3 = nn.InputLayer(40, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 360, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 200, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 200, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "\n",
    "def get_h7_net_3():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(40, \"d1\")\n",
    "    d2 = nn.InputLayer(40, \"d2\")\n",
    "    d3 = nn.InputLayer(40, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "    days = nn.FullConnectLayer(days, 120, d.relu, rng, 0.8)\n",
    "    days = nn.FullConnectLayer(days, 100, d.relu, rng, 0.8)\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 100, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 40, d.relu, rng, 0.5)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 1)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function()\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h7_net_1, train_set1, test_set1, params1)\n",
    "test(get_h7_net_2, train_set4, test_set4, params4)\n",
    "test(get_h7_net_3, train_set4, test_set4, params4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H8 - predicting middle day will grant better accuracy, but worse convergence"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h8_net_1():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)\n",
    "\n",
    "def get_h8_net_2():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1 = nn.InputLayer(120, \"d1\")\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d1, d2, d3])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    middle = nn.MergeLayer([days, city])\n",
    "    middle = nn.FullConnectLayer(middle, 400, d.relu, rng, 0.8)\n",
    "    middle = nn.FullConnectLayer(middle, 250, d.relu, rng, 0.6)\n",
    "    middle = nn.FullConnectLayer(middle, 80, d.relu, rng, 0.6)\n",
    "    middle = nn.FullConnectLayer(middle, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    d2 = nn.InputLayer(120, \"d2\")\n",
    "    d3 = nn.InputLayer(120, \"d3\")\n",
    "    days = nn.MergeLayer([d2, d3, middle])\n",
    "\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    coords = nn.InputLayer(2, \"coords\")\n",
    "    city_one_hot = nn.InputLayer(36, \"city_one_hot\")\n",
    "    city = nn.MergeLayer([date, coords, city_one_hot])\n",
    "\n",
    "    output = nn.MergeLayer([days, city])\n",
    "    output = nn.FullConnectLayer(output, 400, d.relu, rng, 0.8)\n",
    "    output = nn.FullConnectLayer(output, 250, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 80, d.relu, rng, 0.6)\n",
    "    output = nn.FullConnectLayer(output, 1, d.linear, rng, 0.5)\n",
    "\n",
    "    return nn.NeuralNetwork(output, d.l2_loss, rng)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(function, train_set, test_set, params):\n",
    "    print(\"================================== new test ==================================\")\n",
    "    net = function()\n",
    "\n",
    "    start = time.time()\n",
    "    net.train(train_set, test_set, 1024, \"output_temp\")\n",
    "    end = time.time()\n",
    "\n",
    "    print(f\"Time elapsed: {end - start : .2f} s\")\n",
    "\n",
    "    predicted = denormalized(net.predict(train_set), params[\"temperature\"])\n",
    "    expected = denormalized(train_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "    predicted = denormalized(net.predict(test_set), params[\"temperature\"])\n",
    "    expected = denormalized(test_set[\"output_temp\"], params[\"temperature\"])\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(get_h8_net_1, train_set1, test_set1, params1)\n",
    "test(get_h8_net_2, train_set1, test_set1, params1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
