{"cells":[{"cell_type":"code","execution_count":168,"metadata":{},"outputs":[],"source":["import neural_network as nn\n","import definitions as d\n","import numpy as np\n","import os\n","from pandas import read_csv\n","import geopy.distance\n","from sklearn.preprocessing import OneHotEncoder\n","from collections import defaultdict\n","import datetime\n","\n","def load_raw_data(path, type):\n","    files = os.listdir(f\"{path}/{type}\")\n","    data = {}\n","    surfix = f\"_{type}.csv\"\n","    for file in files:\n","        name = file[:file.find(surfix)]\n","        df = read_csv(f\"{path}/{type}/{file}\", sep=\";\")\n","        data[name] = df\n","    return data\n","\n","def get_nearest_cittes(city_attributes):\n","    nerast_cities = {}\n","    for _, row in city_attributes.iterrows():\n","        source = (row[\"Latitude\"], row[\"Longitude\"])\n","        city_dist = []\n","        for _, row2 in city_attributes.iterrows():\n","            if row[\"City\"] is row2[\"City\"]:\n","                continue\n","            destination = (row2[\"Latitude\"], row2[\"Longitude\"])\n","            city_dist.append((row2[\"City\"], geopy.distance.geodesic(source, destination).km))\n","        city_dist.sort(key=lambda x: x[1])\n","        nerast_cities[row[\"City\"]] = [cd[0] for cd in city_dist[:3]]\n","    return nerast_cities\n","    \n","def load_train():\n","    dict = load_raw_data(\"data\", \"train\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[12:, :]\n","    return dict\n","\n","def load_test():\n","    dict = load_raw_data(\"data\", \"test\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[:-1, :]\n","    return dict\n","\n","def get_normalization_params(raw):\n","    params = {}\n","    for key, df in raw.items():\n","        all = np.reshape(df.to_numpy()[:, 1:], -1)\n","        params[key] = (np.nanmean(all), np.nanstd(all))\n","    return params\n","\n","def to_city_time_vect(raw):\n","    cities = next(iter(raw.values())).columns[1:]\n","    hours = next(iter(raw.values()))[[\"datetime\"]]\n","    city_time_dir = {c: hours.copy() for c in cities}\n","    for city in cities:\n","        for key, df in raw.items():\n","            city_time_dir[city][key] = df[[city]]\n","    return city_time_dir\n","\n","def normalize(ctv, params):\n","    for df in ctv.values():\n","        for param, ms in params.items():\n","            mean, std = ms\n","            df[param] = (df[param] - mean) / std \n","\n","def normalize_city_attributes(city_attributes):\n","    la_mean = city_attributes[\"Latitude\"].mean()\n","    la_std = city_attributes[\"Latitude\"].std()\n","    city_attributes[\"Latitude\"] = (city_attributes[\"Latitude\"] - la_mean) / la_std\n","    lo_mean = city_attributes[\"Longitude\"].mean()\n","    lo_std = city_attributes[\"Longitude\"].std()\n","    city_attributes[\"Longitude\"] = (city_attributes[\"Longitude\"] - lo_mean) / lo_std\n","\n","def to_city_day_vect(ctv, wind_treshold):\n","    cdv = {}\n","    for city, df in ctv.items():\n","        u = 0\n","        cdr = []\n","        while u < len(df):\n","            w = u+24\n","            date = df.iloc[u, 0]\n","            vec = np.reshape(df.iloc[u:w, 1:].to_numpy(), -1)\n","            temp_mean = df.iloc[u:w][\"temperature\"].mean()\n","            wind_cat = int(np.any(df.iloc[u:w][\"wind_speed\"].to_numpy() > wind_treshold))\n","            cdr.append((date, vec, temp_mean, wind_cat))\n","            u = w\n","        cdv[city] = cdr\n","    return cdv\n","\n","def get_city_encoder(cities_attr):\n","    cities = np.reshape(city_attributes_raw[\"City\"].to_numpy(), (-1, 1))\n","    cohe = OneHotEncoder()\n","    cohe.fit(cities)\n","    return cohe\n","\n","def get_wind_treshold(souce, params):\n","    mean, std = params[\"wind_speed\"]\n","    return (souce - mean)/std\n","\n","def drop_nan_records(data_set):\n","    mask = [np.any(np.isnan(val), axis=0) for val in data_set.values()]\n","    mask = np.vstack(mask)\n","    mask = np.any(mask, axis = 0)\n","    return {key: val[:, ~mask] for key, val in data_set.items()}\n","\n","def get_set1(cdv, city_encoder, city_attributes_raw):\n","    d1 = []\n","    d2 = []\n","    d3 = []\n","    output_temp = []\n","    output_wind = []\n","    date = []\n","    city_one_hot = []\n","    cord = []\n","\n","    for city, dv in cdv.items():\n","        d1 += [r[1] for r in dv[:-4]]\n","        d2 += [r[1] for r in dv[1:-3]]\n","        d3 += [r[1] for r in dv[2:-2]]\n","        output_temp += [r[2] for r in dv[4:]]\n","        output_wind += [r[3] for r in dv[4:]]\n","        date_str = [r[0] for r in dv[4:]]\n","        date += [datetime.datetime.strptime(d, \"%d.%m.%Y %H:%M\").timetuple().tm_yday / 365 - 0.5 for d in date_str]\n","        size = len(date_str)\n","        city_one_hot += [city_encoder.transform([[city]]).toarray()[0]]*size\n","        cord += [city_attributes_raw.loc[city_attributes_raw[\"City\"] == city][[\"Latitude\", \"Longitude\"]].to_numpy()]*size\n","\n","    set = {\n","        \"d1\": d1,\n","        \"d2\": d2,\n","        \"d3\": d3,\n","        \"output_temp\": output_temp,\n","        \"output_wind\": output_wind,\n","        \"date\": date,\n","        \"city_one_hot\": city_one_hot,\n","        \"cord\": cord\n","    }\n","\n","    return {key: np.vstack(val).T for key, val in set.items()}\n","\n","city_attributes_raw = read_csv(\"data/city_attributes.csv\", sep=\";\")\n","train_raw = load_train()\n","nerast_cities = get_nearest_cittes(city_attributes_raw)\n","normalization_params = get_normalization_params(train_raw)\n","train_ctv = to_city_time_vect(train_raw)\n","normalize(train_ctv, normalization_params)\n","normalize_city_attributes(city_attributes_raw)\n","wind_treshold = get_wind_treshold(6, normalization_params)\n","train_cdv = to_city_day_vect(train_ctv, wind_treshold)\n","city_encoder = get_city_encoder(city_attributes_raw)\n","train_set = get_set1(train_cdv, city_encoder, city_attributes_raw)\n","train_set = drop_nan_records(train_set)\n","\n","test_raw = load_test()\n","test_ctv = to_city_time_vect(test_raw)\n","normalize(test_ctv, normalization_params)\n","test_cdv = to_city_day_vect(test_ctv, wind_treshold)\n","test_set = get_set1(test_cdv, city_encoder, city_attributes_raw)\n","test_set = drop_nan_records(test_set)\n"]},{"cell_type":"code","execution_count":189,"metadata":{},"outputs":[],"source":["import neural_network as nn\n","import definitions as d\n","\n","rng = np.random.default_rng(0)\n","\n","def get_day_layer(num):\n","    l = nn.InputLayer(120, f\"d{num}\")\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_days_layer():\n","    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n","    l = nn.MergeLayer(ls)\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_city_layer():\n","    coh = nn.InputLayer(36, \"city_one_hot\")\n","    date = nn.InputLayer(1, \"date\")\n","    cord = nn.InputLayer(2, \"cord\")\n","    return nn.MergeLayer([coh, date, cord])\n","\n","def get_nn():\n","    ds = get_days_layer()\n","    c = get_city_layer()\n","    l = nn.MergeLayer([ds, c])\n","    l = nn.FullConnectLayer(l, 60, d.relu, rng)\n","    l = nn.FullConnectLayer(l, 1, d.linear, rng)\n","    return nn.NeuralNetwork(l, d.l2_loss)\n","\n","net = get_nn()"]},{"cell_type":"code","execution_count":190,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 0.7304230707869158, test: 0.13082724761831813\n","Train: 0.11612814257231642, test: 0.1285674178759632\n","Train: 0.10612802642849753, test: 0.11514731651606276\n","Train: 0.1007374571051435, test: 0.11213547131728856\n","Train: 0.09658725857278032, test: 0.11016502489013917\n","Train: 0.09435004585178133, test: 0.11171731599489414\n","Train: 0.09188860398254166, test: 0.11308086197756438\n","Train: 0.09018225385852241, test: 0.11387330817783387\n","Train: 0.08848884224937081, test: 0.11467544843545381\n","Train: 0.08697787565571195, test: 0.11441859697945708\n","Train: 0.08566401267493495, test: 0.12187779343010574\n","Train: 0.08501676310552572, test: 0.12292089541896146\n","Train: 0.08360090465821626, test: 0.11566952512062237\n","Train: 0.08155023997378692, test: 0.11665951612418546\n","Train: 0.08024747380548478, test: 0.11549110996832024\n","Train: 0.07937389545252099, test: 0.11107701014485458\n","Train: 0.0778092205444004, test: 0.11251638588814053\n","Train: 0.07653225690009707, test: 0.11373908616195283\n","Train: 0.07499096125571647, test: 0.11538977467396636\n","Train: 0.07424633658300106, test: 0.11510442859988614\n","Train: 0.07294570404463045, test: 0.11377219662255451\n","Train: 0.07080063098484479, test: 0.11345403380057466\n","Train: 0.06996033526536621, test: 0.11711009530510949\n","Train: 0.0686391303468475, test: 0.1139088361191242\n","Train: 0.06815257853645719, test: 0.11374248958606605\n","Train: 0.06616974761424957, test: 0.11486855433693569\n","Train: 0.0652569771351788, test: 0.11738699334313552\n","Train: 0.06539218532784824, test: 0.12087223718460688\n","Train: 0.06632066544774905, test: 0.11593789983988724\n","Train: 0.0699699411371155, test: 0.12028847648242956\n","Train: 0.07113483289065162, test: 0.12486754402522074\n","Train: 0.06838015695029956, test: 0.11755627027977915\n","Train: 0.06281333202845998, test: 0.1193404140810516\n","Train: 0.06101821232739613, test: 0.12106075494797983\n","Train: 0.05919440548912927, test: 0.12222415011840185\n","Train: 0.060396549724769114, test: 0.12368816861041601\n","Train: 0.060511553126124315, test: 0.12878223916275702\n","Train: 0.06298852073832828, test: 0.1308663426071643\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[1;32mIn[190], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m net\u001b[39m.\u001b[39;49mtrain(train_set, test_set, \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_temp\u001b[39;49m\u001b[39m\"\u001b[39;49m, rng)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:194\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[1;34m(self, train_set, test_set, batch_size, output_name, rng)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights()\n\u001b[0;32m    193\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(losses)\n\u001b[1;32m--> 194\u001b[0m predicted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward(test_set)\n\u001b[0;32m    195\u001b[0m test_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(predicted, test_set[output_name])\n\u001b[0;32m    196\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m, test: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:123\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_forward\u001b[39m(\u001b[39mself\u001b[39m, inputs):\n\u001b[0;32m    122\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[1;32m--> 123\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer)\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:135\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:135\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:131\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(layer) \u001b[39m==\u001b[39m MergeLayer:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m mi \u001b[39min\u001b[39;00m layer\u001b[39m.\u001b[39mmerge_inputs:\n\u001b[1;32m--> 131\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(mi\u001b[39m.\u001b[39;49minput_layer)\n\u001b[0;32m    132\u001b[0m         mi\u001b[39m.\u001b[39mpropagate_forward(x)\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:135\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[0;32m    136\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:131\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mtype\u001b[39m(layer) \u001b[39m==\u001b[39m MergeLayer:\n\u001b[0;32m    130\u001b[0m     \u001b[39mfor\u001b[39;00m mi \u001b[39min\u001b[39;00m layer\u001b[39m.\u001b[39mmerge_inputs:\n\u001b[1;32m--> 131\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(mi\u001b[39m.\u001b[39;49minput_layer)\n\u001b[0;32m    132\u001b[0m         mi\u001b[39m.\u001b[39mpropagate_forward(x)\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:136\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[1;34m(self, layer)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    135\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_forward_req(layer\u001b[39m.\u001b[39minput_layer)\n\u001b[1;32m--> 136\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39;49mpropagate_forward(x)\n","File \u001b[1;32md:\\Projects\\nn\\neural_network.py:38\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[0;32m     37\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mW, x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mb\n\u001b[1;32m---> 38\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation[\u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mz)\n","File \u001b[1;32md:\\Projects\\nn\\definitions.py:6\u001b[0m, in \u001b[0;36mrelu_n\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu_n\u001b[39m(x):\n\u001b[1;32m----> 6\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mmaximum(\u001b[39m0\u001b[39;49m, x)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["net.train(train_set, test_set, 1024, \"output_temp\", rng)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT","timestamp":1666562444086}]},"kernelspec":{"display_name":"Python 3.10.8 ('nn')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"0f1f581d7662b2de74cb4bd3e3e3359ed981481056fd35b782ff0281504803e3"}}},"nbformat":4,"nbformat_minor":0}
