{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation__without_mid_prediction,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation__without_mid_prediction\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_nn(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "def get_nn2(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    # coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    # date_layer = nn.InputLayer(1, \"date\")\n",
    "    # cord_layer = nn.InputLayer(2, \"cord\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_set, test_set, params) = get_sets__without_neighbors__one_prediction__without_aggregation__without_mid_prediction()\n",
    "# (train_set, test_set, params) = get_sets__without_neighbors__one_prediction__with_aggregation__without_mid_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_day_layer(num):\n",
    "    l = nn.InputLayer(120, f\"d{num}\")\n",
    "    return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "def get_days_layer():\n",
    "    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "    l = nn.MergeLayer(ls)\n",
    "    return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    l = nn.MergeLayer([coh, date, cord])\n",
    "    return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "def get_nn1(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_days_layer():\n",
    "    ld1 = nn.InputLayer(120, \"d1\")\n",
    "    ld2 = nn.InputLayer(120, \"d2\")\n",
    "    ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "    ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng, ld11)\n",
    "    ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng, ld11)\n",
    "\n",
    "    l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "    return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    l = nn.MergeLayer([coh, date, cord])\n",
    "    return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "def get_nn3(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# best ones for get_sets__without_neighbors__one_prediction__without_aggregation__without_mid_prediction:\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn1([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "\n",
    "# best ones for get_sets__without_neighbors__one_prediction__with_aggregation__without_mid_prediction:\n",
    "net3 = get_nn1([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.99%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: train: 0.017677084673304203, test: 0.014369078538024316\n",
      "Epoch 2/15: train: 0.01445565768162976, test: 0.014365033828265568\n",
      "Epoch 3/15: train: 0.014439904289636644, test: 0.014369319849455059\n",
      "Epoch 4/15: train: 0.014439542840870566, test: 0.014373736423082022\n",
      "Epoch 5/15: train: 0.014439515799399977, test: 0.014377153908846939\n",
      "Epoch 6/15: train: 0.014439732544360146, test: 0.014379642008586959\n",
      "Epoch 7/15: train: 0.014440083300858823, test: 0.014381617969476169\n",
      "Epoch 8/15: train: 0.014440513547725723, test: 0.014383135460859217\n",
      "Epoch 9/15: train: 0.014440995535915765, test: 0.01438414836311623\n",
      "Epoch 10/15: train: 0.014441504635707628, test: 0.014384559390870475\n",
      "Epoch 11/15: train: 0.01444201409640495, test: 0.014384266878245873\n",
      "Epoch 12/15: train: 0.014442493628142322, test: 0.014383211707243125\n",
      "Epoch 13/15: train: 0.014442911313133733, test: 0.014381420949482615\n",
      "Epoch 14/15: train: 0.014443238062569672, test: 0.014379031948331013\n",
      "Epoch 15/15: train: 0.01444345329781733, test: 0.014376280103968334\n"
     ]
    }
   ],
   "source": [
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.31% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59839751 0.59839751 0.59839751 ... 0.59839751 0.59839751 0.59839751]]\n",
      "[[0.5753539  0.57437143 0.57095394 ... 0.64950349 0.61789978 0.57858124]]\n",
      "[[289.5402573 289.5402573 289.5402573 ... 289.5402573 289.5402573\n",
      "  289.5402573]]\n",
      "[[287.7225     287.645      287.37541667 ... 293.57166667 291.07866108\n",
      "  287.97708333]]\n",
      "min: 0.0001593660670096142, max: 41.077444800599636, mean: 7.5016760396423905, median: 6.459381201766291\n",
      "Good predictions: 7393, bad predictions: 38290, success rate:  16.18%\n",
      "[[0.59839751 0.59839751 0.59839751 ... 0.59839751 0.59839751 0.59839751]]\n",
      "[[0.39678851 0.36225967 0.31486372 ... 0.67760934 0.68179801 0.69291554]]\n",
      "[[289.5402573 289.5402573 289.5402573 ... 289.5402573 289.5402573\n",
      "  289.5402573]]\n",
      "[[273.63666667 270.91291667 267.17416667 ... 295.78875    296.11916667\n",
      "  296.99615385]]\n",
      "min: 0.0005760327337043236, max: 38.56240664376631, mean: 7.613466083581378, median: 6.678909366066989\n",
      "Good predictions: 1957, bad predictions: 10454, success rate:  15.77%\n",
      "89.45755704200643\n"
     ]
    }
   ],
   "source": [
    "predicted = net3.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "print(np.average(d.l2_loss_n(predicted, expected)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5: train: 0.5667184855836179, test: 0.7064634393468613\n",
      "Epoch 2/5: train: 0.5567351869560997, test: 0.715061707880622\n",
      "Epoch 3/5: train: 0.556772205210926, test: 0.7151248616412659\n",
      "Epoch 4/5: train: 0.5567751819507388, test: 0.714116552352304\n",
      "Epoch 5/5: train: 0.5567698392420339, test: 0.7131947541093019\n"
     ]
    }
   ],
   "source": [
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24673702 0.24673702 0.24673702 ... 0.24673702 0.24673702 0.24673702]\n",
      " [0.75326298 0.75326298 0.75326298 ... 0.75326298 0.75326298 0.75326298]]\n",
      "[0.24673712 0.75326298]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[1 0 0 ... 0 1 0]\n",
      "0\n",
      "12411\n",
      "Good predictions: 7631, bad predictions: 4780, success_rate:  61.49%\n"
     ]
    }
   ],
   "source": [
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted, axis=1))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
