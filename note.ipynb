{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# (train_set, test_set, params) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "# (train_set, test_set, params) = get_sets__without_neighbors__one_prediction__with_aggregation()\n",
    "(train_set, test_set, params) = get_sets__without_neighbors__24_predictions__without_aggregation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_nn(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "def get_nn2(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    # coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    # date_layer = nn.InputLayer(1, \"date\")\n",
    "    # cord_layer = nn.InputLayer(2, \"cord\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_day_layer(num):\n",
    "    l = nn.InputLayer(120, f\"d{num}\")\n",
    "    return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "def get_days_layer():\n",
    "    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "    l = nn.MergeLayer(ls)\n",
    "    return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    l = nn.MergeLayer([coh, date, cord])\n",
    "    return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "def get_nn1(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_days_layer():\n",
    "    ld1 = nn.InputLayer(120, \"d1\")\n",
    "    ld2 = nn.InputLayer(120, \"d2\")\n",
    "    ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "    ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng, ld11)\n",
    "    ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng, ld11)\n",
    "\n",
    "    l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "    return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    l = nn.MergeLayer([coh, date, cord])\n",
    "    return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "def get_nn3(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_day_layer(num):\n",
    "    l = nn.InputLayer(120, f\"d{num}\")\n",
    "    return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "def get_days_layer():\n",
    "    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "    l = nn.MergeLayer(ls)\n",
    "    return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    l = nn.MergeLayer([coh, date, cord])\n",
    "    return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "def get_nn4(layer_sizes, activations, loss1, loss2):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for i in range(len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 2:\n",
    "            break\n",
    "        l = nn.FullConnectLayer(l, layer_sizes[i], activations[i], rng)\n",
    "    l_temp = nn.FullConnectLayer(l, layer_sizes[-2], activations[-2], rng)\n",
    "    l_wind = nn.FullConnectLayer(l, layer_sizes[-1], activations[-1], rng)\n",
    "    return (nn.NeuralNetwork(l_temp, loss1), nn.NeuralNetwork(l_wind, loss2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# best ones for get_sets__without_neighbors__one_prediction__without_aggregation__without_mid_prediction:\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn1([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.relu, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)  # 15.66% + 61.49%\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "\n",
    "# best ones for get_sets__without_neighbors__one_prediction__with_aggregation__without_mid_prediction:\n",
    "net3 = get_nn1([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.99%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.31% shared weights for days\n",
    "net3.train(train_set, test_set, 1, \"output_temp\", rng, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3_wind = get_nn2([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.sigmoid, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net4_temp.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net4_temp.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "print(np.average(d.l2_loss_n(predicted, expected)))\n",
    "\n",
    "\n",
    "predicted = net4_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net4_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mid day prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_mid_prediction(loss):\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 50, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 1, d.linear, rng)\n",
    "\n",
    "    \n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d2_layer, d3_layer, d4_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 300, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 140, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 50, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 1, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net_mp = get_nn_mid_prediction(d.l2_loss)\n",
    "net_mp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = net_mp.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_mp.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_24_predictions():\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 60, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 24, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(d4_layer, d.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: train: 1.4371287173480618, test: 286.06338508483634\n",
      "Epoch 2/10: train: 329.7410375770448, test: 210.84758010464557\n",
      "Epoch 3/10: train: 196.03537812348856, test: 40.51573460442282\n",
      "Epoch 4/10: train: 37.39850662948772, test: 46.39638672093542\n",
      "Epoch 5/10: train: 47.233543364820605, test: 22.386548168392384\n",
      "Epoch 6/10: train: 22.75393434307843, test: 40.34126578513673\n",
      "Epoch 7/10: train: 40.639740535283956, test: 58.262043550516125\n",
      "Epoch 8/10: train: 58.357720537658025, test: 25.17646505118653\n",
      "Epoch 9/10: train: 24.692720102278237, test: 16.497279280873283\n",
      "Epoch 10/10: train: 16.736280747708378, test: 19.247296999332065\n"
     ]
    }
   ],
   "source": [
    "net_24pred = get_nn_24_predictions()\n",
    "net_24pred.train(train_set, test_set, 512, \"output_temp\", rng, 10)  # only works without batching in nn (too big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[388.8904483  388.21688282 381.65921601 ... 354.25660869 353.51409903\n",
      " 354.19386744]\n",
      "[287.7225     287.645      287.37541667 ... 293.57166667 291.07866108\n",
      " 287.97708333]\n",
      "[train] min: 32.41519191197136, max: 137.84974003050996, mean: 91.22824528834094, median: 91.51163869422555\n",
      "[train] Good predictions: 0, bad predictions: 45662, success rate:  0.00%\n",
      "[390.94545742 384.95822221 378.60983261 ... 372.83841788 372.05623681\n",
      " 375.03447441]\n",
      "[274.52708333 271.91791667 269.57708333 ... 295.43458333 296.45708333\n",
      " 295.81458333]\n",
      "[test] min: 42.90539736025528, max: 134.18359999962672, mean: 92.90506079782628, median: 94.02194857669122\n",
      "[test] Good predictions: 0, bad predictions: 12401, success rate:  0.00%\n"
     ]
    }
   ],
   "source": [
    "predicted = net_24pred.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_24pred.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
