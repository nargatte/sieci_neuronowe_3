{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__without_neighbors__8_predictions__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__without_neighbors__one_prediction__with_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__8_predictions__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_merge_initially(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn_merge_after_while(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.8)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.7)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 39, d.linear, rng, 0.8)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn_only_days(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn3(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "    \n",
    "    def get_days_layer():\n",
    "        ld1 = nn.InputLayer(120, \"d1\")\n",
    "        ld2 = nn.InputLayer(120, \"d2\")\n",
    "        ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "        ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "        ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng)\n",
    "        ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng)\n",
    "\n",
    "        l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn4(layer_sizes, activations, dropout_rates, loss1, loss2):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for i in range(len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 2:\n",
    "            break\n",
    "        l = nn.FullConnectLayer(l, layer_sizes[i], activations[i], rng)\n",
    "    l_temp = nn.FullConnectLayer(l, layer_sizes[-2], activations[-2], rng)\n",
    "    l_wind = nn.FullConnectLayer(l, layer_sizes[-1], activations[-1], rng)\n",
    "    return (nn.NeuralNetwork(l_temp, loss1), nn.NeuralNetwork(l_wind, loss2))\n",
    "\n",
    "\n",
    "def get_nn_mid_prediction(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 50, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 1, d.linear, rng)\n",
    "\n",
    "    \n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d2_layer, d3_layer, d4_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 300, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 140, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 50, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 1, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.relu, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)  # 15.66% + 61.49%\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: train: 1.4137449510220899, test: 0.04933559365976356\n",
      "[train] min: 1.6313242156229535e-05, max: 64.54708513384946, mean: 11.826338078560804, median: 10.440518391948842\n",
      "[train] Good predictions: 4480, bad predictions: 41203, success rate:  9.81%\n",
      "[test] min: 0.0032865260336052415, max: 56.821508878575685, mean: 14.511860371252052, median: 13.326924999309313\n",
      "[test] Good predictions: 956, bad predictions: 11455, success rate:  7.70%\n",
      "Epoch 1/1: train: 0.01743127423435559, test: 0.05388014108709817\n",
      "[train] min: 0.0002173012827029197, max: 66.33764013155206, mean: 5.617858216962235, median: 4.490985273182787\n",
      "[train] Good predictions: 10929, bad predictions: 34754, success rate:  23.92%\n",
      "[test] min: 0.005521322821834929, max: 70.81363200697038, mean: 14.756594316279456, median: 12.682904377549391\n",
      "[test] Good predictions: 987, bad predictions: 11424, success rate:  7.95%\n",
      "Epoch 1/1: train: 0.007416775771008171, test: 0.06538672409546821\n",
      "[train] min: 0.00013065674693280016, max: 67.22029657658152, mean: 4.759822245007676, median: 3.815779250566493\n",
      "[train] Good predictions: 12756, bad predictions: 32927, success rate:  27.92%\n",
      "[test] min: 0.0009215949383474253, max: 73.53728595909178, mean: 16.411973961902273, median: 14.464670358155388\n",
      "[test] Good predictions: 894, bad predictions: 11517, success rate:  7.20%\n",
      "Epoch 1/1: train: 0.006337590261266035, test: 0.06612587640545571\n",
      "[train] min: 0.00014809825194106452, max: 68.49072286330266, mean: 4.4961334870373655, median: 3.572048126409072\n",
      "[train] Good predictions: 13629, bad predictions: 32054, success rate:  29.83%\n",
      "[test] min: 0.0017378513733774525, max: 74.22772145133135, mean: 16.47602355649132, median: 14.415835145374785\n",
      "[test] Good predictions: 874, bad predictions: 11537, success rate:  7.04%\n",
      "Epoch 1/1: train: 0.005903956727146977, test: 0.06589783889654752\n",
      "[train] min: 9.132558756164144e-05, max: 70.41054980277784, mean: 4.2326696574831875, median: 3.230423329975906\n",
      "[train] Good predictions: 14911, bad predictions: 30772, success rate:  32.64%\n",
      "[test] min: 0.0063387755283770275, max: 77.09723947284442, mean: 16.294305822532092, median: 13.955609304724646\n",
      "[test] Good predictions: 869, bad predictions: 11542, success rate:  7.00%\n",
      "Epoch 1/1: train: 0.005596085558223387, test: 0.06801852701799402\n",
      "[train] min: 0.0002050408048717145, max: 70.98411955946085, mean: 4.105070238074215, median: 3.130276033037376\n",
      "[train] Good predictions: 15515, bad predictions: 30168, success rate:  33.96%\n",
      "[test] min: 0.00035679331136861947, max: 78.90669006450565, mean: 16.479807671322448, median: 14.001305497808573\n",
      "[test] Good predictions: 897, bad predictions: 11514, success rate:  7.23%\n",
      "Epoch 1/1: train: 0.005454589409136786, test: 0.07066266933939896\n",
      "[train] min: 1.8453785116889776e-05, max: 70.8707104264987, mean: 4.052225533581573, median: 3.0967713162418136\n",
      "[train] Good predictions: 15562, bad predictions: 30121, success rate:  34.07%\n",
      "[test] min: 0.0023389361214185556, max: 80.93199407767861, mean: 16.72559128970718, median: 14.103706142861142\n",
      "[test] Good predictions: 887, bad predictions: 11524, success rate:  7.15%\n",
      "Epoch 1/1: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[267], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m net3 \u001b[39m=\u001b[39m get_nn_merge_initially([\u001b[39m300\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m60\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], [d\u001b[39m.\u001b[39mlinear, d\u001b[39m.\u001b[39mlinear, d\u001b[39m.\u001b[39mlinear, d\u001b[39m.\u001b[39mlinear, d\u001b[39m.\u001b[39mlinear], [\u001b[39m0.5\u001b[39m, \u001b[39m0.6\u001b[39m, \u001b[39m0.6\u001b[39m, \u001b[39m0.8\u001b[39m, \u001b[39m1\u001b[39m], d\u001b[39m.\u001b[39ml2_loss)\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     net3\u001b[39m.\u001b[39;49mtrain(train_set1, test_set1, \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_temp\u001b[39;49m\u001b[39m\"\u001b[39;49m, rng, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m     predicted \u001b[39m=\u001b[39m net3\u001b[39m.\u001b[39mpredict(train_set1)\n\u001b[1;32m      9\u001b[0m     expected \u001b[39m=\u001b[39m train_set1[\u001b[39m\"\u001b[39m\u001b[39moutput_temp\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:197\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, train_set, test_set, batch_size, output_name, rng, epoch_count)\u001b[0m\n\u001b[1;32m    195\u001b[0m     expected \u001b[39m=\u001b[39m batch[output_name]\n\u001b[1;32m    196\u001b[0m     losses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(predicted, expected))\n\u001b[0;32m--> 197\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward(expected)\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights()\n\u001b[1;32m    199\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(losses)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:146\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward\u001b[0;34m(self, expected)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_backward\u001b[39m(\u001b[39mself\u001b[39m, expected):\n\u001b[1;32m    145\u001b[0m     da \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss[\u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma, expected)\n\u001b[0;32m--> 146\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer, da)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:158\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mpropagate_backward(da)\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(layer\u001b[39m.\u001b[39;49minput_layer, nda)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:158\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mpropagate_backward(da)\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(layer\u001b[39m.\u001b[39;49minput_layer, nda)\n",
      "    \u001b[0;31m[... skipping similar frames: NeuralNetwork.propagate_backward_req at line 158 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:158\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mpropagate_backward(da)\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(layer\u001b[39m.\u001b[39;49minput_layer, nda)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:157\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(mi\u001b[39m.\u001b[39minput_layer, nda)\n\u001b[1;32m    156\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 157\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mpropagate_backward(da)\n\u001b[1;32m    158\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(layer\u001b[39m.\u001b[39minput_layer, nda)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:65\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_backward\u001b[0;34m(self, da)\u001b[0m\n\u001b[1;32m     63\u001b[0m db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39msum(dz, \u001b[39m1\u001b[39m), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mupdate_derivative(dW, db)\n\u001b[0;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mW\u001b[39m.\u001b[39;49mT, dz)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid], [0.5, 0.6, 0.6, 0.8, 1], d.l2_loss)\n",
    "\n",
    "while True:\n",
    "    net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 1)\n",
    "\n",
    "    predicted = net3.predict(train_set1)\n",
    "    expected = train_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "    predicted = net3.predict(test_set1)\n",
    "    expected = test_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: train: 0.0060670772907663924, test: 0.05497609616386401\n",
      "Epoch 2/15: train: 0.001929655409781732, test: 0.05577942432225474\n",
      "Epoch 3/15: train: 0.001812920854232353, test: 0.05483961363233612\n",
      "Epoch 4/15: train: 0.0017941756806227696, test: 0.053025281826781415\n",
      "Epoch 5/15: train: 0.0017651678758746803, test: 0.05240972642452739\n",
      "Epoch 6/15: train: 0.001744873728354971, test: 0.05185866461908931\n",
      "Epoch 7/15: train: 0.0018048783106294605, test: 0.05170222381353928\n",
      "Epoch 8/15: train: 0.00178732141717938, test: 0.05168748776216245\n",
      "Epoch 9/15: train: 0.0017694443631623914, test: 0.053164428502811716\n",
      "Epoch 10/15: train: 0.0017555506643470646, test: 0.05158265230959261\n",
      "Epoch 11/15: train: 0.0017634852975898273, test: 0.05361702952787019\n",
      "Epoch 12/15: train: 0.001816516218349976, test: 0.05098295941423114\n",
      "Epoch 13/15: train: 0.001807289489042563, test: 0.05285344664434598\n",
      "Epoch 14/15: train: 0.0017283817263267329, test: 0.05231572507352986\n",
      "Epoch 15/15: train: 0.0017385934560919666, test: 0.05128770539706222\n"
     ]
    }
   ],
   "source": [
    "# rng = np.random.default_rng(1)\n",
    "\n",
    "# net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] min: 2.7148055721681885e-05, max: 25.38636795286976, mean: 2.351503048060779, median: 1.6902239127363714\n",
      "[train] Good predictions: 25781, bad predictions: 19902, success rate:  56.43%\n",
      "[test] min: 0.0005824673033885119, max: 60.784248791217436, mean: 14.128784784049726, median: 11.564250715051116\n",
      "[test] Good predictions: 1161, bad predictions: 11250, success rate:  9.35%\n"
     ]
    }
   ],
   "source": [
    "predicted = net3.predict(train_set1)\n",
    "expected = train_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "predicted = net3.predict(test_set1)\n",
    "expected = test_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~17.61% shared weights for days\n",
    "net3.train(train_set, test_set, 1, \"output_temp\", rng, 2)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 24 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_24_predictions():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 60, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 24, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(d4_layer, d.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_24pred = get_nn_24_predictions()\n",
    "net_24pred.train(train_set, test_set, 512, \"output_temp\", rng, 10)  # only works without batching in nn (too big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = net_24pred.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_24pred.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
