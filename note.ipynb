{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__without_neighbors__8_predictions__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__without_neighbors__one_prediction__with_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__8_predictions__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_merge_initially(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn_merge_after_while(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.8)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.7)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 39, d.linear, rng, 0.8)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn_only_days(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn3(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "    \n",
    "    def get_days_layer():\n",
    "        ld1 = nn.InputLayer(120, \"d1\")\n",
    "        ld2 = nn.InputLayer(120, \"d2\")\n",
    "        ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "        ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "        ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng)\n",
    "        ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng)\n",
    "\n",
    "        l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn4(layer_sizes, activations, dropout_rates, loss1, loss2):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for i in range(len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 2:\n",
    "            break\n",
    "        l = nn.FullConnectLayer(l, layer_sizes[i], activations[i], rng)\n",
    "    l_temp = nn.FullConnectLayer(l, layer_sizes[-2], activations[-2], rng)\n",
    "    l_wind = nn.FullConnectLayer(l, layer_sizes[-1], activations[-1], rng)\n",
    "    return (nn.NeuralNetwork(l_temp, loss1), nn.NeuralNetwork(l_wind, loss2))\n",
    "\n",
    "\n",
    "def get_nn_mid_prediction(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 50, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 1, d.linear, rng)\n",
    "\n",
    "    \n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d2_layer, d3_layer, d4_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 300, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 140, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 50, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 1, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.relu, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)  # 15.66% + 61.49%\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: train: 0.08742700627252124, test: 0.04041009392645985\n",
      "[train] min: 0.0017969044552046398, max: 39.60188022308989, mean: 14.23410124706065, median: 14.523046898649397\n",
      "[train] Good predictions: 2427, bad predictions: 43256, success rate:  5.31%\n",
      "[test] min: 0.0032031054146273163, max: 36.71096356624719, mean: 13.903480722172882, median: 14.266380257563412\n",
      "[test] Good predictions: 882, bad predictions: 11529, success rate:  7.11%\n",
      "Epoch 1/1: train: 0.024133177931014327, test: 0.015305317459894817\n",
      "[train] min: 0.000511232052701871, max: 38.205198719121285, mean: 8.112455494737139, median: 7.384422337633055\n",
      "[train] Good predictions: 6398, bad predictions: 39285, success rate:  14.01%\n",
      "[test] min: 0.0003220596178152846, max: 37.85447622560622, mean: 8.12979117468888, median: 7.620738739232536\n",
      "[test] Good predictions: 1689, bad predictions: 10722, success rate:  13.61%\n",
      "Epoch 1/1: train: 0.014723016872641721, test: 0.014286578109348384\n",
      "[train] min: 5.130460374402901e-05, max: 40.743488799401774, mean: 7.539810781632841, median: 6.536384632197496\n",
      "[train] Good predictions: 7359, bad predictions: 38324, success rate:  16.11%\n",
      "[test] min: 0.0013013864361823835, max: 40.39276631700926, mean: 7.6203300478000315, median: 6.733384713498992\n",
      "[test] Good predictions: 1938, bad predictions: 10473, success rate:  15.62%\n",
      "Epoch 1/1: train: 0.014444569333332017, test: 0.01429565075912798\n",
      "[train] min: 0.0005791702879491822, max: 40.96153608513325, mean: 7.513846133673859, median: 6.486234719940512\n",
      "[train] Good predictions: 7399, bad predictions: 38284, success rate:  16.20%\n",
      "[test] min: 0.0010153344297805234, max: 40.61081360441398, mean: 7.599302783994091, median: 6.664818059800666\n",
      "[test] Good predictions: 1988, bad predictions: 10423, success rate:  16.02%\n",
      "Epoch 1/1: train: 0.01442260704374895, test: 0.01429431695761528\n",
      "[train] min: 1.1491115571971022e-05, max: 40.94344898239697, mean: 7.515839416731535, median: 6.493321852740166\n",
      "[train] Good predictions: 7405, bad predictions: 38278, success rate:  16.21%\n",
      "[test] min: 0.0012384620737293517, max: 40.592726502266316, mean: 7.600908641542151, median: 6.67998846791221\n",
      "[test] Good predictions: 1980, bad predictions: 10431, success rate:  15.95%\n",
      "Epoch 1/1: train: 0.014432042630729834, test: 0.0142929196527167\n",
      "[train] min: 0.0011888789365457342, max: 40.9227513377958, mean: 7.518167114515755, median: 6.494019483571549\n",
      "[train] Good predictions: 7389, bad predictions: 38294, success rate:  16.17%\n",
      "[test] min: 0.0015194556082747113, max: 40.57202885831981, mean: 7.602788750980851, median: 6.691692594704477\n",
      "[test] Good predictions: 1969, bad predictions: 10442, success rate:  15.86%\n",
      "Epoch 1/1: train: 0.01443696555552334, test: 0.01429367063093944\n",
      "[train] min: 6.516521386856766e-05, max: 40.93412766408406, mean: 7.516881018764168, median: 6.490976499711849\n",
      "[train] Good predictions: 7395, bad predictions: 38288, success rate:  16.19%\n",
      "[test] min: 0.0001431313610851248, max: 40.583405185489596, mean: 7.601747989999242, median: 6.685976468581259\n",
      "[test] Good predictions: 1972, bad predictions: 10439, success rate:  15.89%\n",
      "Epoch 1/1: train: 0.014437590355076623, test: 0.01429551796181696\n",
      "[train] min: 0.000346960444232991, max: 40.959797222482706, mean: 7.514036920844316, median: 6.486962731179233\n",
      "[train] Good predictions: 7397, bad predictions: 38286, success rate:  16.19%\n",
      "[test] min: 0.0007235254246324985, max: 40.60907474491222, mean: 7.599455756643595, median: 6.664890230029243\n",
      "[test] Good predictions: 1988, bad predictions: 10423, success rate:  16.02%\n",
      "Epoch 1/1: train: 0.01444040255117528, test: 0.014294450913903738\n",
      "[train] min: 4.299664709606077e-05, max: 40.94533199291928, mean: 7.515630467850597, median: 6.492311177520548\n",
      "[train] Good predictions: 7406, bad predictions: 38277, success rate:  16.21%\n",
      "[test] min: 0.00018879064259635925, max: 40.594609516235266, mean: 7.600740183028531, median: 6.678105453032003\n",
      "[test] Good predictions: 1982, bad predictions: 10429, success rate:  15.97%\n",
      "Epoch 1/1: train: 0.01444141104899932, test: 0.014298705373320899\n",
      "[train] min: 0.00014681425159324135, max: 40.998519828951004, mean: 7.509843468674987, median: 6.4790306499617145\n",
      "[train] Good predictions: 7401, bad predictions: 38282, success rate:  16.20%\n",
      "[test] min: 0.0028342986219627164, max: 40.64779735356632, mean: 7.596095751674259, median: 6.6486676175948105\n",
      "[test] Good predictions: 1988, bad predictions: 10423, success rate:  16.02%\n",
      "Epoch 1/1: train: 0.01444037310144774, test: 0.014292449704342873\n",
      "[train] min: 0.0005015728746684545, max: 40.915290060272554, mean: 7.519014991901994, median: 6.497314082758123\n",
      "[train] Good predictions: 7389, bad predictions: 38294, success rate:  16.17%\n",
      "[test] min: 0.0022692646752489054, max: 40.564567585562656, mean: 7.603476476707342, median: 6.694814078464901\n",
      "[test] Good predictions: 1966, bad predictions: 10445, success rate:  15.84%\n",
      "Epoch 1/1: train: 0.014445810418283125, test: 0.014293676772246177\n",
      "[train] min: 0.00015566347718731777, max: 40.93421815993847, mean: 7.5168708415469405, median: 6.490886000145963\n",
      "[train] Good predictions: 7395, bad predictions: 38288, success rate:  16.19%\n",
      "[test] min: 5.2629188701303065e-05, max: 40.5834956864997, mean: 7.601739786587912, median: 6.685885967060301\n",
      "[test] Good predictions: 1972, bad predictions: 10439, success rate:  15.89%\n",
      "Epoch 1/1: train: 0.014420680312318305, test: 0.014291787150738682\n",
      "[train] min: 0.0004275744386745828, max: 40.904249656750295, mean: 7.520279663030095, median: 6.500646164808018\n",
      "[train] Good predictions: 7387, bad predictions: 38296, success rate:  16.17%\n",
      "[test] min: 0.0013781668251340307, max: 40.55352718440645, mean: 7.6044991324245546, median: 6.6958544182649575\n",
      "[test] Good predictions: 1965, bad predictions: 10446, success rate:  15.83%\n",
      "Epoch 1/1: train: 0.014428860221968303, test: 0.014294802652168289\n",
      "[train] min: 0.00024375333970283464, max: 40.950202055674225, mean: 7.5150922469124115, median: 6.4920974121577615\n",
      "[train] Good predictions: 7398, bad predictions: 38285, success rate:  16.19%\n",
      "[test] min: 0.001347942562915705, max: 40.59947958490915, mean: 7.600306738180125, median: 6.6736520565560795\n",
      "[test] Good predictions: 1984, bad predictions: 10427, success rate:  15.99%\n",
      "Epoch 1/1: train: 0.014439643220257248, test: 0.014294342518584767\n",
      "[train] min: 0.00037202552226744956, max: 40.94380951086728, mean: 7.5157993785989365, median: 6.49364477189647\n",
      "[train] Good predictions: 7404, bad predictions: 38279, success rate:  16.21%\n",
      "[test] min: 0.000877919438266872, max: 40.59308704149544, mean: 7.6008763672024955, median: 6.679627925778561\n",
      "[test] Good predictions: 1980, bad predictions: 10431, success rate:  15.95%\n",
      "Epoch 1/1: train: 0.014438871573305, test: 0.014293414262211868\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m     net3\u001b[39m.\u001b[39mtrain(train_set1, test_set1, \u001b[39m1024\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutput_temp\u001b[39m\u001b[39m\"\u001b[39m, rng, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     predicted \u001b[39m=\u001b[39m net3\u001b[39m.\u001b[39;49mpredict(train_set1)\n\u001b[1;32m      9\u001b[0m     expected \u001b[39m=\u001b[39m train_set1[\u001b[39m\"\u001b[39m\u001b[39moutput_temp\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m     predicted \u001b[39m=\u001b[39m denormalized(predicted, params1[\u001b[39m\"\u001b[39m\u001b[39mtemperature\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:204\u001b[0m, in \u001b[0;36mNeuralNetwork.predict\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m--> 204\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward(data)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:128\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward\u001b[0;34m(self, inputs, is_training)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_forward\u001b[39m(\u001b[39mself\u001b[39m, inputs, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    127\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[0;32m--> 128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer, is_training)\n\u001b[1;32m    129\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:140\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:140\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "    \u001b[0;31m[... skipping similar frames: NeuralNetwork.propagate_forward_req at line 140 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:140\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    141\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:141\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    140\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_forward_req(layer\u001b[39m.\u001b[39minput_layer)\n\u001b[0;32m--> 141\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39;49mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:39\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_forward\u001b[0;34m(self, x, is_training)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_forward\u001b[39m(\u001b[39mself\u001b[39m, x, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m     38\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights\u001b[39m.\u001b[39;49mW, x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mb\n\u001b[1;32m     40\u001b[0m     r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrng\u001b[39m.\u001b[39mbinomial(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz\u001b[39m.\u001b[39mshape)\n\u001b[1;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m is_training:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid], [0.8, 0.6, 0.6, 0.5, 1], d.l2_loss)\n",
    "\n",
    "while True:\n",
    "    net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 1)\n",
    "\n",
    "    predicted = net3.predict(train_set1)\n",
    "    expected = train_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "    predicted = net3.predict(test_set1)\n",
    "    expected = test_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: train: 0.0060670772907663924, test: 0.05497609616386401\n",
      "Epoch 2/15: train: 0.001929655409781732, test: 0.05577942432225474\n",
      "Epoch 3/15: train: 0.001812920854232353, test: 0.05483961363233612\n",
      "Epoch 4/15: train: 0.0017941756806227696, test: 0.053025281826781415\n",
      "Epoch 5/15: train: 0.0017651678758746803, test: 0.05240972642452739\n",
      "Epoch 6/15: train: 0.001744873728354971, test: 0.05185866461908931\n",
      "Epoch 7/15: train: 0.0018048783106294605, test: 0.05170222381353928\n",
      "Epoch 8/15: train: 0.00178732141717938, test: 0.05168748776216245\n",
      "Epoch 9/15: train: 0.0017694443631623914, test: 0.053164428502811716\n",
      "Epoch 10/15: train: 0.0017555506643470646, test: 0.05158265230959261\n",
      "Epoch 11/15: train: 0.0017634852975898273, test: 0.05361702952787019\n",
      "Epoch 12/15: train: 0.001816516218349976, test: 0.05098295941423114\n",
      "Epoch 13/15: train: 0.001807289489042563, test: 0.05285344664434598\n",
      "Epoch 14/15: train: 0.0017283817263267329, test: 0.05231572507352986\n",
      "Epoch 15/15: train: 0.0017385934560919666, test: 0.05128770539706222\n"
     ]
    }
   ],
   "source": [
    "# rng = np.random.default_rng(1)\n",
    "\n",
    "# net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] min: 2.7148055721681885e-05, max: 25.38636795286976, mean: 2.351503048060779, median: 1.6902239127363714\n",
      "[train] Good predictions: 25781, bad predictions: 19902, success rate:  56.43%\n",
      "[test] min: 0.0005824673033885119, max: 60.784248791217436, mean: 14.128784784049726, median: 11.564250715051116\n",
      "[test] Good predictions: 1161, bad predictions: 11250, success rate:  9.35%\n"
     ]
    }
   ],
   "source": [
    "predicted = net3.predict(train_set1)\n",
    "expected = train_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "predicted = net3.predict(test_set1)\n",
    "expected = test_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~17.61% shared weights for days\n",
    "net3.train(train_set, test_set, 1, \"output_temp\", rng, 2)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 24 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_24_predictions():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 60, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 24, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(d4_layer, d.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_24pred = get_nn_24_predictions()\n",
    "net_24pred.train(train_set, test_set, 512, \"output_temp\", rng, 10)  # only works without batching in nn (too big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = net_24pred.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_24pred.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov  1 2022, 14:18:21) [GCC 12.2.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
