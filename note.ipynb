{"cells":[{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["import datetime\n","import definitions as d\n","import geopy.distance\n","import neural_network as nn\n","import numpy as np\n","import os\n","import pandas as pd\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","def load_raw_data(path, type):\n","    files = os.listdir(f\"{path}/{type}\")\n","    data = {}\n","    surfix = f\"_{type}.csv\"\n","    for file in files:\n","        name = file[:file.find(surfix)]\n","        df = pd.read_csv(f\"{path}/{type}/{file}\", sep=\";\")\n","        data[name] = df\n","    return data\n","\n","def get_nearest_cities(city_attributes):\n","    nearest_cities = {}\n","    for _, row in city_attributes.iterrows():\n","        source = (row[\"Latitude\"], row[\"Longitude\"])\n","        city_dist = []\n","        for _, row2 in city_attributes.iterrows():\n","            if row[\"City\"] is row2[\"City\"]:\n","                continue\n","            destination = (row2[\"Latitude\"], row2[\"Longitude\"])\n","            city_dist.append((row2[\"City\"], geopy.distance.geodesic(source, destination).km))\n","        city_dist.sort(key=lambda x: x[1])\n","        nearest_cities[row[\"City\"]] = [cd[0] for cd in city_dist[:3]]\n","    return nearest_cities\n","    \n","def load_train():\n","    dict = load_raw_data(\"data\", \"train\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[12:, :]\n","    return dict\n","\n","def load_test():\n","    dict = load_raw_data(\"data\", \"test\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[:-1, :]\n","    return dict\n","\n","def get_normalization_params(raw):\n","    params = {}\n","    for key, df in raw.items():\n","        all = np.reshape(df.to_numpy()[:, 1:], -1)\n","        params[key] = (np.nanmean(all), np.nanstd(all))\n","    return params\n","\n","def to_city_time_vect(raw):\n","    cities = next(iter(raw.values())).columns[1:]\n","    hours = next(iter(raw.values()))[[\"datetime\"]]\n","    ctvs = {c: hours.copy() for c in cities}\n","    for city in cities:\n","        for key, df in raw.items():\n","            ctvs[city][key] = df[[city]]\n","    return ctvs\n","\n","def normalize(ctv, params):\n","    for df in ctv.values():\n","        for param, ms in params.items():\n","            mean, std = ms\n","            df[param] = (df[param] - mean) / std \n","\n","def normalize_city_attributes(city_attributes):\n","    latitude_mean = city_attributes[\"Latitude\"].mean()\n","    latitude_std = city_attributes[\"Latitude\"].std()\n","\n","    longitude_mean = city_attributes[\"Longitude\"].mean()\n","    longitude_std = city_attributes[\"Longitude\"].std()\n","\n","    city_attributes[\"Latitude\"] = (city_attributes[\"Latitude\"] - latitude_mean) / latitude_std\n","    city_attributes[\"Longitude\"] = (city_attributes[\"Longitude\"] - longitude_mean) / longitude_std\n","\n","def to_city_day_vect(ctv, wind_treshold):\n","    cdv = {}\n","    for city, df in ctv.items():\n","        u = 0\n","        cdr = []\n","        while u < len(df):\n","            w = u + 24\n","            date = df.iloc[u, 0]\n","            vec = np.reshape(df.iloc[u : w, 1:].to_numpy(), -1)\n","            temp_mean = df.iloc[u : w][\"temperature\"].mean()\n","            wind_cat = int(np.any(df.iloc[u:w][\"wind_speed\"].to_numpy() > wind_treshold))\n","            cdr.append((date, vec, temp_mean, wind_cat))\n","            u = w\n","        cdv[city] = cdr\n","    return cdv\n","\n","def get_city_encoder(cities_attr):\n","    cities = np.reshape(city_attributes_raw[\"City\"].to_numpy(), (-1, 1))\n","    cohe = OneHotEncoder()\n","    cohe.fit(cities)\n","    return cohe\n","\n","def get_wind_treshold(souce, params):\n","    mean, std = params[\"wind_speed\"]\n","    return (souce - mean) / std\n","\n","def drop_nan_records(data_set):\n","    mask = [np.any(np.isnan(val), axis=0) for val in data_set.values()]\n","    mask = np.vstack(mask)\n","    mask = np.any(mask, axis=0)\n","    return {key: val[:, ~mask] for key, val in data_set.items()}\n","\n","def get_set1(cdv, city_encoder, city_attributes_raw):\n","    d1 = []\n","    d2 = []\n","    d3 = []\n","    output_temp = []\n","    output_wind = []\n","    date = []\n","    city_one_hot = []\n","    cord = []\n","\n","    for city, dv in cdv.items():\n","        d1 += [r[1] for r in dv[:-4]]\n","        d2 += [r[1] for r in dv[1:-3]]\n","        d3 += [r[1] for r in dv[2:-2]]\n","        output_temp += [r[2] for r in dv[4:]]\n","        output_wind += [np.hstack((r[3], 1 - r[3])) for r in dv[4:]]\n","        date_str = [r[0] for r in dv[4:]]\n","        date += [datetime.datetime.strptime(d, \"%d.%m.%Y %H:%M\").timetuple().tm_yday / 365 for d in date_str]\n","        size = len(date_str)\n","        city_one_hot += [city_encoder.transform([[city]]).toarray()[0]] * size\n","        cord += [city_attributes_raw.loc[city_attributes_raw[\"City\"] == city][[\"Latitude\", \"Longitude\"]].to_numpy()] * size\n","\n","    set = {\n","        \"d1\": d1,\n","        \"d2\": d2,\n","        \"d3\": d3,\n","        \"output_temp\": output_temp,\n","        \"output_wind\": output_wind,\n","        \"date\": date,\n","        \"city_one_hot\": city_one_hot,\n","        \"cord\": cord\n","    }\n","\n","    return {key: np.vstack(val).T for key, val in set.items()}\n","\n","city_attributes_raw = pd.read_csv(\"data/city_attributes.csv\", sep=\";\")\n","\n","train_raw = load_train()\n","nearest_cities = get_nearest_cities(city_attributes_raw)\n","normalization_params = get_normalization_params(train_raw)\n","train_ctv = to_city_time_vect(train_raw)\n","# normalize(train_ctv, normalization_params)\n","# normalize_city_attributes(city_attributes_raw)\n","# wind_treshold = get_wind_treshold(6, normalization_params)\n","wind_treshold = 6\n","train_cdv = to_city_day_vect(train_ctv, wind_treshold)\n","city_encoder = get_city_encoder(city_attributes_raw)\n","train_set = get_set1(train_cdv, city_encoder, city_attributes_raw)\n","train_set = drop_nan_records(train_set)\n","\n","test_raw = load_test()\n","test_ctv = to_city_time_vect(test_raw)\n","# normalize(test_ctv, normalization_params)\n","test_cdv = to_city_day_vect(test_ctv, wind_treshold)\n","test_set = get_set1(test_cdv, city_encoder, city_attributes_raw)\n","test_set = drop_nan_records(test_set)"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import neural_network as nn\n","import definitions as d\n","\n","rng = np.random.default_rng(1)\n","\n","def get_day_layer(num):\n","    l = nn.InputLayer(120, f\"d{num}\")\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_days_layer():\n","    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n","    l = nn.MergeLayer(ls)\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_city_layer():\n","    coh = nn.InputLayer(36, \"city_one_hot\")\n","    date = nn.InputLayer(1, \"date\")\n","    cord = nn.InputLayer(2, \"cord\")\n","    return nn.MergeLayer([coh, date, cord])\n","\n","def get_nn1(layer_sizes, activations, loss):\n","    assert len(layer_sizes) == len(activations)\n","\n","    ds = get_days_layer()\n","    c = get_city_layer()\n","    l = nn.MergeLayer([ds, c])\n","    for (n, activation) in zip(layer_sizes, activations):\n","        l = nn.FullConnectLayer(l, n, activation, rng)\n","    return nn.NeuralNetwork(l, loss)"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20: train: 31.62385197723188, test: 23485.307328350653\n","Epoch 2/20: train: 29.260145815524726, test: 22968.990651420903\n","Epoch 3/20: train: 27.1651623521407, test: 22339.535305635425\n","Epoch 4/20: train: 26.449341842627373, test: 21530.771097953664\n","Epoch 5/20: train: 24.877987234367993, test: 20405.465636839526\n","Epoch 6/20: train: 29.20296240047368, test: 19827.367007643672\n","Epoch 7/20: train: 33.525290962831605, test: 19278.692918999786\n","Epoch 8/20: train: 36.58663769006845, test: 19235.179602484375\n","Epoch 9/20: train: 34.9239913500306, test: 19839.166240569146\n","Epoch 10/20: train: 28.658604745497037, test: 20432.111005334416\n","Epoch 11/20: train: 36.67612266776217, test: 20659.092374308962\n","Epoch 12/20: train: 30.923622115089483, test: 21629.354503028753\n","Epoch 13/20: train: 30.463701492078346, test: 22472.5428982891\n","Epoch 14/20: train: 31.02414121921332, test: 23147.360456611328\n","Epoch 15/20: train: 33.68983248021466, test: 24200.10973756478\n","Epoch 16/20: train: 25.162787326400096, test: 25394.816474688454\n","Epoch 17/20: train: 37.35052029016002, test: 25650.790149646447\n","Epoch 18/20: train: 26.338620680250546, test: 27426.37297666085\n","Epoch 19/20: train: 33.42136145652787, test: 28103.71156731786\n","Epoch 20/20: train: 28.58153279615514, test: 29348.418021520545\n"]}],"source":["# net = get_nn1([60, 1], [d.relu, d.linear], d.l2_loss)\n","net.train(train_set, test_set, 1024, \"output_temp\", rng, 20)"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[286.65846424 286.88144833 284.45409479 ... 292.25359568 290.80737977\n","  290.01535794]]\n","[[287.7225     287.645      287.37541667 ... 293.57166667 291.07866108\n","  287.97708333]]\n","0.00038540078105597786 32.72176587866312\n","1.4853376203855777e-07 1070.713962218042\n"]}],"source":["predicted = net.propagate_forward(train_set)\n","print(predicted)\n","print(train_set[\"output_temp\"])\n","diffs = np.abs(predicted - train_set[\"output_temp\"])\n","print(np.min(diffs), np.max(diffs))\n","losses = d.l2_loss_n(predicted, train_set[\"output_temp\"])\n","print(np.nanmin(losses), np.nanmax(losses))"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[117.03424806 124.66707031 132.21020137 ... 126.113584   128.05420615\n","  125.42073462]]\n","[[274.52708333 271.91791667 269.57708333 ... 296.45708333 295.81458333\n","  294.15      ]]\n","111.30106701928415 220.15223261733348\n","12387.927519631181 48467.00552639651\n"]}],"source":["predicted = net.propagate_forward(test_set)\n","print(predicted)\n","print(test_set[\"output_temp\"])\n","diffs = np.abs(predicted - test_set[\"output_temp\"])\n","print(np.min(diffs), np.max(diffs))\n","losses = d.l2_loss_n(predicted, test_set[\"output_temp\"])\n","print(np.nanmin(losses), np.nanmax(losses))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["net2 = get_nn1([60, 20, 2], [d.relu, d.sigmoid, d.softmax], d.cross_entropy_loss)\n","net2.train(train_set, test_set, 1024, \"output_wind\", rng)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["rng = np.random.default_rng(1)\n","\n","def get_nn(layer_sizes, activations, loss):\n","    assert len(layer_sizes) == len(activations)\n","\n","    d1_layer = nn.InputLayer(120, \"d1\")\n","    d2_layer = nn.InputLayer(120, \"d2\")\n","    d3_layer = nn.InputLayer(120, \"d3\")\n","    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n","\n","    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n","    date_layer = nn.InputLayer(1, \"date\")\n","    cord_layer = nn.InputLayer(2, \"cord\")\n","    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n","\n","    output_layer = nn.MergeLayer([days_layer, city_layer])\n","    for (n, activation) in zip(layer_sizes, activations):\n","        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n","    return nn.NeuralNetwork(output_layer, loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# best ones:\n","net4 = get_nn([300, 120, 60, 1], [d.relu, d.sigmoid, d.relu, d.relu], d.l2_loss)            # mixed sigmoid and relu, no linear, L2 loss - 97,89% success rate\n","net4.train(train_set, test_set, 1024, \"output_temp\", rng, 50)\n","\n","net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # only relu, linear at the end, L2 loss - 97,39% success rate\n","net3.train(train_set, test_set, 1024, \"output_temp\", rng, 50)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50: train: 1.5878878735642612, test: 1.2696588783292804\n","Epoch 2/50: train: 1.073142299259767, test: 0.9473935972291473\n","Epoch 3/50: train: 0.8872722097740278, test: 0.8492196216065322\n","Epoch 4/50: train: 0.8403739133888967, test: 0.8277357501197931\n","Epoch 5/50: train: 0.8324270134271632, test: 0.8240305903352165\n","Epoch 6/50: train: 0.8315304871326011, test: 0.8234106671070144\n","Epoch 7/50: train: 0.8314578697860792, test: 0.8233027890730198\n","Epoch 8/50: train: 0.8314526037595962, test: 0.8232884858338345\n","Epoch 9/50: train: 0.8314529315076024, test: 0.8232894837671444\n","Epoch 10/50: train: 0.8314539379388374, test: 0.8232921521093747\n","Epoch 11/50: train: 0.8314550088150423, test: 0.823294893179031\n","Epoch 12/50: train: 0.8314560743979078, test: 0.8232976474284389\n","Epoch 13/50: train: 0.831457135169456, test: 0.8233004331575566\n","Epoch 14/50: train: 0.8314581949820992, test: 0.8233032592306062\n","Epoch 15/50: train: 0.8314592567603903, test: 0.823306132494718\n","Epoch 16/50: train: 0.8314603229537547, test: 0.8233090592429787\n","Epoch 17/50: train: 0.83146139569565, test: 0.8233120453179242\n","Epoch 18/50: train: 0.8314624768696443, test: 0.8233150962072368\n","Epoch 19/50: train: 0.8314635681588872, test: 0.8233182171229926\n","Epoch 20/50: train: 0.831464671083499, test: 0.82332141305909\n","Epoch 21/50: train: 0.8314657870286953, test: 0.8233246888327291\n","Epoch 22/50: train: 0.8314669172660819, test: 0.8233280491135532\n","Epoch 23/50: train: 0.8314680629697915, test: 0.8233314984428425\n","Epoch 24/50: train: 0.8314692252286363, test: 0.8233350412444079\n","Epoch 25/50: train: 0.831470405055124, test: 0.82333868182829\n","Epoch 26/50: train: 0.8314716033919427, test: 0.823342424387996\n","Epoch 27/50: train: 0.8314728211163659, test: 0.8233462729917195\n","Epoch 28/50: train: 0.8314740590429073, test: 0.8233502315677832\n","Epoch 29/50: train: 0.8314753179244677, test: 0.8233543038843986\n","Epoch 30/50: train: 0.8314765984521603, test: 0.8233584935236993\n","Epoch 31/50: train: 0.8314779012539459, test: 0.8233628038499381\n","Epoch 32/50: train: 0.8314792268921867, test: 0.8233672379716572\n","Epoch 33/50: train: 0.8314805758601946, test: 0.8233717986976079\n","Epoch 34/50: train: 0.8314819485778357, test: 0.823376488486172\n","Epoch 35/50: train: 0.8314833453862477, test: 0.8233813093880322\n","Epoch 36/50: train: 0.8314847665417096, test: 0.8233862629818657\n","Epoch 37/50: train: 0.8314862122087111, test: 0.8233913503028727\n","Epoch 38/50: train: 0.8314876824522726, test: 0.8233965717640274\n","Epoch 39/50: train: 0.8314891772295654, test: 0.82340192707004\n","Epoch 40/50: train: 0.8314906963808983, test: 0.8234074151241477\n","Epoch 41/50: train: 0.8314922396201532, test: 0.8234130339280347\n","Epoch 42/50: train: 0.8314938065247635, test: 0.8234187804753826\n","Epoch 43/50: train: 0.8314953965253551, test: 0.8234246506398211\n","Epoch 44/50: train: 0.8314970088951981, test: 0.8234306390583412\n","Epoch 45/50: train: 0.831498642739638, test: 0.8234367390115926\n","Epoch 46/50: train: 0.831500296985713, test: 0.8234429423028734\n","Epoch 47/50: train: 0.8315019703721959, test: 0.823449239138063\n","Epoch 48/50: train: 0.8315036614403346, test: 0.8234556180092168\n","Epoch 49/50: train: 0.8315053685255998, test: 0.823462065585035\n","Epoch 50/50: train: 0.8315070897507835, test: 0.8234685666119138\n"]}],"source":["net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.relu], d.l2_loss)\n","net3.train(train_set, test_set, 1024, \"output_temp\", rng, 50)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["min: 4.0568124653472815e-05, max: 3.909551528784863, mean: 0.7308811093893285, median: 0.6395367614404732\n","Good predictions: 12078, bad predictions: 333, success rate:  97.32%\n"]}],"source":["predicted = net3.predict(test_set)\n","# print(predicted)\n","# print(test_set[\"output_temp\"])\n","diffs = np.abs(predicted - test_set[\"output_temp\"])\n","print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n","print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/50: train: 0.78906344605059, test: 1.4343300022512013\n","Epoch 2/50: train: 0.12255857600500575, test: 1.5126048290925336\n","Epoch 3/50: train: 0.11337302371947092, test: 1.634899399760955\n","Epoch 4/50: train: 0.10698878177431567, test: 1.7508230918626693\n","Epoch 5/50: train: 0.10115390794197508, test: 1.7716065690906664\n","Epoch 6/50: train: 0.0983728152001137, test: 1.825186244539546\n","Epoch 7/50: train: 0.09428762490506061, test: 1.8867489169359895\n","Epoch 8/50: train: 0.09172895606472399, test: 1.9894601105198915\n","Epoch 9/50: train: 0.09087384109804907, test: 1.9670954647260688\n","Epoch 10/50: train: 0.088397059143718, test: 2.0500542740075525\n","Epoch 11/50: train: 0.08642064059158985, test: 2.126180877508857\n","Epoch 12/50: train: 0.08475278970750069, test: 2.2158284956124294\n","Epoch 13/50: train: 0.08339711285610989, test: 2.146735890450285\n","Epoch 14/50: train: 0.08388790243780016, test: 2.037982049629908\n","Epoch 15/50: train: 0.0833981688153626, test: 2.1152646587024537\n","Epoch 16/50: train: 0.08032817965627269, test: 2.158032481219793\n","Epoch 17/50: train: 0.07769926785722871, test: 2.2801331672865572\n","Epoch 18/50: train: 0.07579688378836485, test: 2.2081829195720695\n","Epoch 19/50: train: 0.07378249459028458, test: 2.393851426979355\n","Epoch 20/50: train: 0.07226637538834656, test: 2.457242368328207\n","Epoch 21/50: train: 0.07047656630299357, test: 2.84014756235197\n","Epoch 22/50: train: 0.0706935651588113, test: 2.622602824101109\n","Epoch 23/50: train: 0.07109179765129983, test: 2.35622396769781\n","Epoch 24/50: train: 0.07098303255649344, test: 2.321187525251373\n","Epoch 25/50: train: 0.06831895771811407, test: 2.425803684293627\n","Epoch 26/50: train: 0.06736495437114666, test: 2.6290707580242874\n","Epoch 27/50: train: 0.07382549489068074, test: 2.5442069429699363\n","Epoch 28/50: train: 0.07258043837935886, test: 2.4986617859417226\n","Epoch 29/50: train: 0.0677371712295724, test: 2.322576234187896\n","Epoch 30/50: train: 0.06596406353335461, test: 1.97640047490636\n","Epoch 31/50: train: 0.06507281686929246, test: 2.089092800433522\n","Epoch 32/50: train: 0.0636404207092832, test: 2.382645995315846\n","Epoch 33/50: train: 0.06149597004735386, test: 2.20669151061913\n","Epoch 34/50: train: 0.06964031065780556, test: 2.372846213308158\n","Epoch 35/50: train: 0.06215997932709863, test: 2.346263231793347\n","Epoch 36/50: train: 0.05598653702297904, test: 2.3721522465247515\n","Epoch 37/50: train: 0.05317232284583773, test: 2.3294713722768217\n","Epoch 38/50: train: 0.05084941135080622, test: 2.2753007202321767\n","Epoch 39/50: train: 0.04938100060740904, test: 2.3510902050979037\n","Epoch 40/50: train: 0.04899508996648047, test: 2.4111709210943855\n","Epoch 41/50: train: 0.04825205151767287, test: 2.535644981999552\n","Epoch 42/50: train: 0.052528445322749, test: 2.369052551955777\n","Epoch 43/50: train: 0.052753071934938285, test: 2.281570015258294\n","Epoch 44/50: train: 0.054560504313010356, test: 2.4664873668615557\n","Epoch 45/50: train: 0.0665305499389368, test: 2.706442524026299\n","Epoch 46/50: train: 0.07048827609964854, test: 1.9945974740562862\n","Epoch 47/50: train: 0.06362165359779828, test: 2.133410020486013\n","Epoch 48/50: train: 0.059914533684697784, test: 2.0974727479645012\n","Epoch 49/50: train: 0.05599961765197472, test: 2.144495476868417\n","Epoch 50/50: train: 0.05065888779766536, test: 2.177371976345942\n"]}],"source":["net4 = get_nn([300, 120, 60, 60, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)\n","net4.train(train_set, test_set, 1024, \"output_temp\", rng, 50)"]},{"cell_type":"code","execution_count":123,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["min: 0.00010808493156189147, max: 4.872049940411021, mean: 1.2025532296535677, median: 1.0479134727794892\n","Good predictions: 10239, bad predictions: 2172, success rate:  82.50%\n"]}],"source":["predicted = net4.predict(test_set)\n","# print(predicted)\n","# print(test_set[\"output_temp\"])\n","diffs = np.abs(predicted - test_set[\"output_temp\"])\n","print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n","print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[{"ename":"AssertionError","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m net5 \u001b[39m=\u001b[39m get_nn([\u001b[39m300\u001b[39;49m, \u001b[39m140\u001b[39;49m, \u001b[39m100\u001b[39;49m, \u001b[39m40\u001b[39;49m, \u001b[39m1\u001b[39;49m], [d\u001b[39m.\u001b[39;49mrelu, d\u001b[39m.\u001b[39;49mrelu, d\u001b[39m.\u001b[39;49msigmoid, d\u001b[39m.\u001b[39;49mrelu], d\u001b[39m.\u001b[39;49ml2_loss)\n\u001b[1;32m      2\u001b[0m net5\u001b[39m.\u001b[39mtrain(train_set, test_set, \u001b[39m1024\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39moutput_temp\u001b[39m\u001b[39m\"\u001b[39m, rng, \u001b[39m50\u001b[39m)\n","Cell \u001b[0;32mIn[63], line 4\u001b[0m, in \u001b[0;36mget_nn\u001b[0;34m(layer_sizes, activations, loss)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_nn\u001b[39m(layer_sizes, activations, loss):\n\u001b[0;32m----> 4\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(layer_sizes) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(activations)\n\u001b[1;32m      6\u001b[0m     d1_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mInputLayer(\u001b[39m120\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39md1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m     d2_layer \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mInputLayer(\u001b[39m120\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39md2\u001b[39m\u001b[39m\"\u001b[39m)\n","\u001b[0;31mAssertionError\u001b[0m: "]}],"source":["net5 = get_nn([300, 140, 100, 40, 1], [d.relu, d.relu, d.relu, d.linear], d.l2_loss)\n","net5.train(train_set, test_set, 1024, \"output_temp\", rng, 50)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["predicted = net5.predict(test_set)\n","diffs = np.abs(predicted - test_set[\"output_temp\"])\n","print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n","print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10: train: 0.5566873834380162, test: 0.720929590507864\n","Epoch 2/10: train: 0.5566442246647492, test: 0.7213692962454761\n","Epoch 3/10: train: 0.5566431902821923, test: 0.7213631926210489\n","Epoch 4/10: train: 0.5566433132408876, test: 0.7213190798676528\n","Epoch 5/10: train: 0.5566434326888592, test: 0.7212776756469556\n","Epoch 6/10: train: 0.5566435274256281, test: 0.7212416731512793\n","Epoch 7/10: train: 0.5566436067554377, test: 0.7212103608394359\n","Epoch 8/10: train: 0.5566436749625224, test: 0.721182940728686\n","Epoch 9/10: train: 0.5566437344419763, test: 0.7211587669686064\n","Epoch 10/10: train: 0.5566437868531428, test: 0.7211373248017345\n"]}],"source":["net3_wind = get_nn([300, 100, 60, 2, 2, 2], [d.sigmoid, d.sigmoid, d.relu, d.relu, d.sigmoid, d.softmax], d.cross_entropy_loss)\n","net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 10)"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.24681464 0.24681445 0.24681449 ... 0.24681599 0.24681769 0.24681711]\n"," [0.75318536 0.75318555 0.75318551 ... 0.75318401 0.75318231 0.75318289]]\n","[0.2468225  0.75319455]\n","[1 1 1 ... 1 1 1]\n","[0 1 0 ... 1 0 0]\n","12411\n","12411\n","Good predictions: 4869, bad predictions: 7542, success_rate:  39.23%\n"]}],"source":["predicted = net3_wind.predict(test_set)\n","print(predicted)\n","print(np.max(predicted, axis=1))\n","predicted = np.argmax(predicted, axis=0)\n","expected = test_set[\"output_wind\"][0, :]\n","print(predicted)\n","print(expected)\n","print(np.count_nonzero(predicted == 1))\n","print(predicted.size)\n","print(f\"Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["def get_nn2(layer_sizes, activations, loss):\n","    assert len(layer_sizes) == len(activations)\n","\n","    d1_layer = nn.InputLayer(120, \"d1\")\n","    d2_layer = nn.InputLayer(120, \"d2\")\n","    d3_layer = nn.InputLayer(120, \"d3\")\n","    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n","    date_layer = nn.InputLayer(1, \"date\")\n","    cord_layer = nn.InputLayer(2, \"cord\")\n","\n","    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer, coh_layer, date_layer, cord_layer])\n","    for (n, activation) in zip(layer_sizes, activations):\n","        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n","    return nn.NeuralNetwork(output_layer, loss)"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/1: train: 1.1834330314038481, test: 1.1077163803938357\n"]}],"source":["# wn = get_nn2([2, 2], [d.sigmoid, d.softmax], d.hinge_loss)\n","wn.train(train_set, test_set, 1024, \"output_wind\", rng, 1)"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[0.39857106 0.3983715  0.40396978 ... 0.40522713 0.39855062 0.39825823]\n"," [0.60142894 0.6016285  0.59603022 ... 0.59477287 0.60144938 0.60174177]]\n","[0.88309602 0.6017803 ]\n","[1 1 1 ... 1 1 1]\n","[0 1 0 ... 1 0 0]\n","3744\n","12411\n","Good predictions: 7000, bad predictions: 5411, success_rate:  56.40%\n"]}],"source":["predicted = wn.predict(test_set)\n","print(predicted)\n","print(np.max(predicted, axis=1))\n","predicted = np.argmax(predicted, axis=0)\n","expected = test_set[\"output_wind\"][0, :]\n","print(predicted)\n","print(expected)\n","print(np.count_nonzero(predicted == 1))\n","print(predicted.size)\n","print(f\"Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1.2 0.8]\n"," [1.2 1.2]]\n"]}],"source":["print(d.hinge_loss_n(np.asarray([[0.6, 0.4], [0.4, 0.6]]), np.asarray([[0.0, 1.0], [1.0, 0.0]])))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT","timestamp":1666562444086}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}
