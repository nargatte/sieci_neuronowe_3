{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import datetime\n",
    "import definitions as d\n",
    "import geopy.distance\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "def load_raw_data(path, type):\n",
    "    files = os.listdir(f\"{path}/{type}\")\n",
    "    data = {}\n",
    "    surfix = f\"_{type}.csv\"\n",
    "    for file in files:\n",
    "        name = file[:file.find(surfix)]\n",
    "        df = pd.read_csv(f\"{path}/{type}/{file}\", sep=\";\")\n",
    "        data[name] = df\n",
    "    return data\n",
    "\n",
    "def get_nearest_cities(city_attributes):\n",
    "    nearest_cities = {}\n",
    "    for _, row in city_attributes.iterrows():\n",
    "        source = (row[\"Latitude\"], row[\"Longitude\"])\n",
    "        city_dist = []\n",
    "        for _, row2 in city_attributes.iterrows():\n",
    "            if row[\"City\"] is row2[\"City\"]:\n",
    "                continue\n",
    "            destination = (row2[\"Latitude\"], row2[\"Longitude\"])\n",
    "            city_dist.append((row2[\"City\"], geopy.distance.geodesic(source, destination).km))\n",
    "        city_dist.sort(key=lambda x: x[1])\n",
    "        nearest_cities[row[\"City\"]] = [cd[0] for cd in city_dist[:3]]\n",
    "    return nearest_cities\n",
    "    \n",
    "def load_train():\n",
    "    dict = load_raw_data(\"data\", \"train\")\n",
    "    dict.pop(\"weather_description\")\n",
    "    for key, df in dict.items():\n",
    "        dict[key] = df.iloc[12:, :]\n",
    "    return dict\n",
    "\n",
    "def load_test():\n",
    "    dict = load_raw_data(\"data\", \"test\")\n",
    "    dict.pop(\"weather_description\")\n",
    "    for key, df in dict.items():\n",
    "        dict[key] = df.iloc[:-1, :]\n",
    "    return dict\n",
    "\n",
    "def get_normalization_params(raw):\n",
    "    params = {}\n",
    "    for key, df in raw.items():\n",
    "        all = np.reshape(df.to_numpy()[:, 1:], -1)\n",
    "        params[key] = (np.nanmean(all), np.nanstd(all))\n",
    "    return params\n",
    "\n",
    "def to_city_time_vect(raw):\n",
    "    cities = next(iter(raw.values())).columns[1:]\n",
    "    hours = next(iter(raw.values()))[[\"datetime\"]]\n",
    "    ctvs = {c: hours.copy() for c in cities}\n",
    "    for city in cities:\n",
    "        for key, df in raw.items():\n",
    "            ctvs[city][key] = df[[city]]\n",
    "    return ctvs\n",
    "\n",
    "def normalize(ctv, params):\n",
    "    for df in ctv.values():\n",
    "        for param, ms in params.items():\n",
    "            mean, std = ms\n",
    "            df[param] = (df[param] - mean) / std \n",
    "\n",
    "def normalize_city_attributes(city_attributes):\n",
    "    latitude_mean = city_attributes[\"Latitude\"].mean()\n",
    "    latitude_std = city_attributes[\"Latitude\"].std()\n",
    "\n",
    "    longitude_mean = city_attributes[\"Longitude\"].mean()\n",
    "    longitude_std = city_attributes[\"Longitude\"].std()\n",
    "\n",
    "    city_attributes[\"Latitude\"] = (city_attributes[\"Latitude\"] - latitude_mean) / latitude_std\n",
    "    city_attributes[\"Longitude\"] = (city_attributes[\"Longitude\"] - longitude_mean) / longitude_std\n",
    "\n",
    "def to_city_day_vect(ctv, wind_treshold):\n",
    "    cdv = {}\n",
    "    for city, df in ctv.items():\n",
    "        u = 0\n",
    "        cdr = []\n",
    "        while u < len(df):\n",
    "            w = u + 24\n",
    "            date = df.iloc[u, 0]\n",
    "            vec = np.reshape(df.iloc[u : w, 1:].to_numpy(), -1)\n",
    "            temp_mean = df.iloc[u : w][\"temperature\"].mean()\n",
    "            wind_cat = int(np.any(df.iloc[u:w][\"wind_speed\"].to_numpy() > wind_treshold))\n",
    "            cdr.append((date, vec, temp_mean, wind_cat))\n",
    "            u = w\n",
    "        cdv[city] = cdr\n",
    "    return cdv\n",
    "\n",
    "def get_city_encoder(cities_attr):\n",
    "    cities = np.reshape(city_attributes_raw[\"City\"].to_numpy(), (-1, 1))\n",
    "    cohe = OneHotEncoder()\n",
    "    cohe.fit(cities)\n",
    "    return cohe\n",
    "\n",
    "def get_wind_treshold(souce, params):\n",
    "    mean, std = params[\"wind_speed\"]\n",
    "    return (souce - mean) / std\n",
    "\n",
    "def drop_nan_records(data_set):\n",
    "    mask = [np.any(np.isnan(val), axis=0) for val in data_set.values()]\n",
    "    mask = np.vstack(mask)\n",
    "    mask = np.any(mask, axis=0)\n",
    "    return {key: val[:, ~mask] for key, val in data_set.items()}\n",
    "\n",
    "def get_set1(cdv, city_encoder, city_attributes_raw):\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    d3 = []\n",
    "    output_temp = []\n",
    "    output_wind = []\n",
    "    date = []\n",
    "    city_one_hot = []\n",
    "    cord = []\n",
    "\n",
    "    for city, dv in cdv.items():\n",
    "        d1 += [r[1] for r in dv[:-4]]\n",
    "        d2 += [r[1] for r in dv[1:-3]]\n",
    "        d3 += [r[1] for r in dv[2:-2]]\n",
    "        output_temp += [r[2] for r in dv[4:]]\n",
    "        output_wind += [np.hstack((r[3], 1 - r[3])) for r in dv[4:]]\n",
    "        date_str = [r[0] for r in dv[4:]]\n",
    "        date += [datetime.datetime.strptime(d, \"%d.%m.%Y %H:%M\").timetuple().tm_yday / 365 for d in date_str]\n",
    "        size = len(date_str)\n",
    "        city_one_hot += [city_encoder.transform([[city]]).toarray()[0]] * size\n",
    "        cord += [city_attributes_raw.loc[city_attributes_raw[\"City\"] == city][[\"Latitude\", \"Longitude\"]].to_numpy()] * size\n",
    "\n",
    "    set = {\n",
    "        \"d1\": d1,\n",
    "        \"d2\": d2,\n",
    "        \"d3\": d3,\n",
    "        \"output_temp\": output_temp,\n",
    "        \"output_wind\": output_wind,\n",
    "        \"date\": date,\n",
    "        \"city_one_hot\": city_one_hot,\n",
    "        \"cord\": cord\n",
    "    }\n",
    "\n",
    "    return {key: np.vstack(val).T for key, val in set.items()}\n",
    "\n",
    "city_attributes_raw = pd.read_csv(\"data/city_attributes.csv\", sep=\";\")\n",
    "\n",
    "train_raw = load_train()\n",
    "nearest_cities = get_nearest_cities(city_attributes_raw)\n",
    "normalization_params = get_normalization_params(train_raw)\n",
    "train_ctv = to_city_time_vect(train_raw)\n",
    "normalize(train_ctv, normalization_params)\n",
    "normalize_city_attributes(city_attributes_raw)\n",
    "wind_treshold = get_wind_treshold(6, normalization_params)\n",
    "# wind_treshold = 6\n",
    "train_cdv = to_city_day_vect(train_ctv, wind_treshold)\n",
    "city_encoder = get_city_encoder(city_attributes_raw)\n",
    "train_set = get_set1(train_cdv, city_encoder, city_attributes_raw)\n",
    "train_set = drop_nan_records(train_set)\n",
    "\n",
    "test_raw = load_test()\n",
    "test_ctv = to_city_time_vect(test_raw)\n",
    "normalize(test_ctv, normalization_params)\n",
    "test_cdv = to_city_day_vect(test_ctv, wind_treshold)\n",
    "test_set = get_set1(test_cdv, city_encoder, city_attributes_raw)\n",
    "test_set = drop_nan_records(test_set)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import neural_network as nn\n",
    "import definitions as d\n",
    "\n",
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_day_layer(num):\n",
    "    l = nn.InputLayer(120, f\"d{num}\")\n",
    "    return nn.FullConnectLayer(l, 60, d.relu, rng)\n",
    "\n",
    "def get_days_layer():\n",
    "    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "    l = nn.MergeLayer(ls)\n",
    "    return nn.FullConnectLayer(l, 60, d.relu, rng)\n",
    "\n",
    "def get_city_layer():\n",
    "    coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date = nn.InputLayer(1, \"date\")\n",
    "    cord = nn.InputLayer(2, \"cord\")\n",
    "    return nn.MergeLayer([coh, date, cord])\n",
    "\n",
    "def get_nn1(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net2 = get_nn1([60, 20, 2], [d.relu, d.sigmoid, d.softmax], d.hinge_loss)\n",
    "net2.train(train_set, test_set, 1024, \"output_wind\", rng, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "def get_nn(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    cord_layer = nn.InputLayer(2, \"cord\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, cord_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# best ones:\n",
    "net4 = get_nn([300, 120, 60, 1], [d.relu, d.sigmoid, d.relu, d.relu], d.l2_loss)            # mixed sigmoid and relu, no linear, L2 loss - ~97,89% success rate\n",
    "net4.train(train_set, test_set, 1024, \"output_temp\", rng, 50)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # only relu, linear at the end, L2 loss - ~98,09% success rate in 5 iterations\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l1_loss)  # only relu, linear at the end, L1 loss - ~97,84% success rate in 5 iterations\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn([300, 100, 60, 2, 2, 2], [d.sigmoid, d.sigmoid, d.relu, d.relu, d.sigmoid, d.softmax], d.hinge_loss)  # hinge and cross_entropy pretty much the same - 60,77% (all predictions no wind)\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 10)\n",
    "\n",
    "net2 = get_nn1([60, 20, 2], [d.relu, d.sigmoid, d.softmax], d.cross_entropy_loss)  # ~60,74%, 60,77% with hinge (all predictions no wind)\n",
    "net2.train(train_set, test_set, 1024, \"output_wind\", rng, 2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net3 = get_nn([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l1_loss)\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)  # 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = net3.predict(test_set)\n",
    "# print(predicted)\n",
    "# print(test_set[\"output_temp\"])\n",
    "diffs = np.abs(predicted - test_set[\"output_temp\"])\n",
    "print(f\"min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "net3_wind = get_nn([300, 100, 60, 2, 2, 2], [d.sigmoid, d.sigmoid, d.relu, d.relu, d.sigmoid, d.softmax], d.hinge_loss)\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicted = net2.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted, axis=1))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}