{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__without_neighbors__8_predictions__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__without_neighbors__one_prediction__with_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__8_predictions__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_merge_initially(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn_merge_after_while(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.8)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng, 0.7)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 39, d.linear, rng, 0.8)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn_only_days(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations) and len(layer_sizes) == len(dropout_rates)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation, dropout_rate) in zip(layer_sizes, activations, dropout_rates):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng, dropout_rate)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn3(layer_sizes, activations, dropout_rates, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "    \n",
    "    def get_days_layer():\n",
    "        ld1 = nn.InputLayer(120, \"d1\")\n",
    "        ld2 = nn.InputLayer(120, \"d2\")\n",
    "        ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "        ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "        ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng)\n",
    "        ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng)\n",
    "\n",
    "        l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn4(layer_sizes, activations, dropout_rates, loss1, loss2):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for i in range(len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 2:\n",
    "            break\n",
    "        l = nn.FullConnectLayer(l, layer_sizes[i], activations[i], rng)\n",
    "    l_temp = nn.FullConnectLayer(l, layer_sizes[-2], activations[-2], rng)\n",
    "    l_wind = nn.FullConnectLayer(l, layer_sizes[-1], activations[-1], rng)\n",
    "    return (nn.NeuralNetwork(l_temp, loss1), nn.NeuralNetwork(l_wind, loss2))\n",
    "\n",
    "\n",
    "def get_nn_mid_prediction(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 50, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 1, d.linear, rng)\n",
    "\n",
    "    \n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d2_layer, d3_layer, d4_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 300, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 140, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 50, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 1, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.relu, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)  # 15.66% + 61.49%\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: train: 0.08742700627252124, test: 0.04041009392645985\n",
      "[train] min: 0.0017969044552046398, max: 39.60188022308989, mean: 14.23410124706065, median: 14.523046898649397\n",
      "[train] Good predictions: 2427, bad predictions: 43256, success rate:  5.31%\n",
      "[test] min: 0.0032031054146273163, max: 36.71096356624719, mean: 13.903480722172882, median: 14.266380257563412\n",
      "[test] Good predictions: 882, bad predictions: 11529, success rate:  7.11%\n",
      "Epoch 1/1: train: 0.024133177931014327, test: 0.015305317459894817\n",
      "[train] min: 0.000511232052701871, max: 38.205198719121285, mean: 8.112455494737139, median: 7.384422337633055\n",
      "[train] Good predictions: 6398, bad predictions: 39285, success rate:  14.01%\n",
      "[test] min: 0.0003220596178152846, max: 37.85447622560622, mean: 8.12979117468888, median: 7.620738739232536\n",
      "[test] Good predictions: 1689, bad predictions: 10722, success rate:  13.61%\n",
      "Epoch 1/1: train: 0.014723016872641721, test: 0.014286578109348384\n",
      "[train] min: 5.130460374402901e-05, max: 40.743488799401774, mean: 7.539810781632841, median: 6.536384632197496\n",
      "[train] Good predictions: 7359, bad predictions: 38324, success rate:  16.11%\n",
      "[test] min: 0.0013013864361823835, max: 40.39276631700926, mean: 7.6203300478000315, median: 6.733384713498992\n",
      "[test] Good predictions: 1938, bad predictions: 10473, success rate:  15.62%\n",
      "Epoch 1/1: train: 0.014444569333332017, test: 0.01429565075912798\n",
      "[train] min: 0.0005791702879491822, max: 40.96153608513325, mean: 7.513846133673859, median: 6.486234719940512\n",
      "[train] Good predictions: 7399, bad predictions: 38284, success rate:  16.20%\n",
      "[test] min: 0.0010153344297805234, max: 40.61081360441398, mean: 7.599302783994091, median: 6.664818059800666\n",
      "[test] Good predictions: 1988, bad predictions: 10423, success rate:  16.02%\n",
      "Epoch 1/1: train: 0.01442260704374895, test: 0.01429431695761528\n",
      "[train] min: 1.1491115571971022e-05, max: 40.94344898239697, mean: 7.515839416731535, median: 6.493321852740166\n",
      "[train] Good predictions: 7405, bad predictions: 38278, success rate:  16.21%\n",
      "[test] min: 0.0012384620737293517, max: 40.592726502266316, mean: 7.600908641542151, median: 6.67998846791221\n",
      "[test] Good predictions: 1980, bad predictions: 10431, success rate:  15.95%\n",
      "Epoch 1/1: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[269], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m net3 \u001b[39m=\u001b[39m get_nn_merge_initially([\u001b[39m300\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m60\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], [d\u001b[39m.\u001b[39msigmoid, d\u001b[39m.\u001b[39msigmoid, d\u001b[39m.\u001b[39msigmoid, d\u001b[39m.\u001b[39msigmoid, d\u001b[39m.\u001b[39msigmoid], [\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m], d\u001b[39m.\u001b[39ml2_loss)\n\u001b[1;32m      5\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m----> 6\u001b[0m     net3\u001b[39m.\u001b[39;49mtrain(train_set1, test_set1, \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_temp\u001b[39;49m\u001b[39m\"\u001b[39;49m, rng, \u001b[39m1\u001b[39;49m)\n\u001b[1;32m      8\u001b[0m     predicted \u001b[39m=\u001b[39m net3\u001b[39m.\u001b[39mpredict(train_set1)\n\u001b[1;32m      9\u001b[0m     expected \u001b[39m=\u001b[39m train_set1[\u001b[39m\"\u001b[39m\u001b[39moutput_temp\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:194\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, train_set, test_set, batch_size, output_name, rng, epoch_count)\u001b[0m\n\u001b[1;32m    192\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m    193\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_batched:\n\u001b[0;32m--> 194\u001b[0m     predicted \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward(batch, \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    195\u001b[0m     expected \u001b[39m=\u001b[39m batch[output_name]\n\u001b[1;32m    196\u001b[0m     losses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(predicted, expected))\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:129\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward\u001b[0;34m(self, inputs, is_training)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_forward\u001b[39m(\u001b[39mself\u001b[39m, inputs, is_training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    128\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minputs \u001b[39m=\u001b[39m inputs\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer, is_training)\n\u001b[1;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:141\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:141\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "    \u001b[0;31m[... skipping similar frames: NeuralNetwork.propagate_forward_req at line 141 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:141\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward()\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_forward_req(layer\u001b[39m.\u001b[39;49minput_layer)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:142\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_forward_req\u001b[0;34m(self, layer, is_training)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    141\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_forward_req(layer\u001b[39m.\u001b[39minput_layer)\n\u001b[0;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m layer\u001b[39m.\u001b[39;49mpropagate_forward(x, is_training)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:45\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_forward\u001b[0;34m(self, x, is_training)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout_rate \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mW, x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mb\n\u001b[0;32m---> 45\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactivation[\u001b[39m\"\u001b[39;49m\u001b[39mn\u001b[39;49m\u001b[39m\"\u001b[39;49m](\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mz)\n",
      "File \u001b[0;32m~/projects/sieci_neuronowe_3/definitions.py:30\u001b[0m, in \u001b[0;36msigmoid_n\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msigmoid_n\u001b[39m(x):\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m np\u001b[39m.\u001b[39;49mexp(\u001b[39m-\u001b[39;49mx))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rng = np.random.default_rng(1)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid], [0.8, 0.6, 0.6, 0.5, 1], d.l2_loss)\n",
    "\n",
    "while True:\n",
    "    net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 1)\n",
    "\n",
    "    predicted = net3.predict(train_set1)\n",
    "    expected = train_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "    predicted = net3.predict(test_set1)\n",
    "    expected = test_set1[\"output_temp\"]\n",
    "\n",
    "    predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "    expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "    diffs = np.abs(predicted - expected)\n",
    "    print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "    print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15: train: 0.0060670772907663924, test: 0.05497609616386401\n",
      "Epoch 2/15: train: 0.001929655409781732, test: 0.05577942432225474\n",
      "Epoch 3/15: train: 0.001812920854232353, test: 0.05483961363233612\n",
      "Epoch 4/15: train: 0.0017941756806227696, test: 0.053025281826781415\n",
      "Epoch 5/15: train: 0.0017651678758746803, test: 0.05240972642452739\n",
      "Epoch 6/15: train: 0.001744873728354971, test: 0.05185866461908931\n",
      "Epoch 7/15: train: 0.0018048783106294605, test: 0.05170222381353928\n",
      "Epoch 8/15: train: 0.00178732141717938, test: 0.05168748776216245\n",
      "Epoch 9/15: train: 0.0017694443631623914, test: 0.053164428502811716\n",
      "Epoch 10/15: train: 0.0017555506643470646, test: 0.05158265230959261\n",
      "Epoch 11/15: train: 0.0017634852975898273, test: 0.05361702952787019\n",
      "Epoch 12/15: train: 0.001816516218349976, test: 0.05098295941423114\n",
      "Epoch 13/15: train: 0.001807289489042563, test: 0.05285344664434598\n",
      "Epoch 14/15: train: 0.0017283817263267329, test: 0.05231572507352986\n",
      "Epoch 15/15: train: 0.0017385934560919666, test: 0.05128770539706222\n"
     ]
    }
   ],
   "source": [
    "# rng = np.random.default_rng(1)\n",
    "\n",
    "# net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set1, test_set1, 1024, \"output_temp\", rng, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[train] min: 2.7148055721681885e-05, max: 25.38636795286976, mean: 2.351503048060779, median: 1.6902239127363714\n",
      "[train] Good predictions: 25781, bad predictions: 19902, success rate:  56.43%\n",
      "[test] min: 0.0005824673033885119, max: 60.784248791217436, mean: 14.128784784049726, median: 11.564250715051116\n",
      "[test] Good predictions: 1161, bad predictions: 11250, success rate:  9.35%\n"
     ]
    }
   ],
   "source": [
    "predicted = net3.predict(train_set1)\n",
    "expected = train_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "\n",
    "predicted = net3.predict(test_set1)\n",
    "expected = test_set1[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params1[\"temperature\"])\n",
    "expected = denormalized(expected, params1[\"temperature\"])\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~17.61% shared weights for days\n",
    "net3.train(train_set, test_set, 1, \"output_temp\", rng, 2)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 24 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_24_predictions():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 60, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 24, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(d4_layer, d.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_24pred = get_nn_24_predictions()\n",
    "net_24pred.train(train_set, test_set, 512, \"output_temp\", rng, 10)  # only works without batching in nn (too big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = net_24pred.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_24pred.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
