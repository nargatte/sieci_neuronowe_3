{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import definitions as d\n",
    "import neural_network as nn\n",
    "import numpy as np\n",
    "\n",
    "from test_case_creator import (\n",
    "    denormalized,\n",
    "    get_sets__without_neighbors__one_prediction__without_aggregation,\n",
    "    get_sets__without_neighbors__one_prediction__with_aggregation,\n",
    "    get_sets__without_neighbors__24_predictions__without_aggregation,\n",
    "    get_sets__without_neighbors__8_predictions__with_aggregation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "(train_set1, test_set1, params1) = get_sets__without_neighbors__one_prediction__without_aggregation()\n",
    "(train_set2, test_set2, params2) = get_sets__without_neighbors__one_prediction__with_aggregation()\n",
    "(train_set3, test_set3, params3) = get_sets__without_neighbors__24_predictions__without_aggregation()\n",
    "(train_set4, test_set4, params4) = get_sets__without_neighbors__8_predictions__with_aggregation()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_nn_merge_initially(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn_merge_after_while(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 120, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 39, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn_only_days(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "    output_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        output_layer = nn.FullConnectLayer(output_layer, n, activation, rng)\n",
    "    return nn.NeuralNetwork(output_layer, loss)\n",
    "\n",
    "\n",
    "def get_nn3(layer_sizes, activations, loss):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "    \n",
    "    def get_days_layer():\n",
    "        ld1 = nn.InputLayer(120, \"d1\")\n",
    "        ld2 = nn.InputLayer(120, \"d2\")\n",
    "        ld3 = nn.InputLayer(120, \"d3\")\n",
    "\n",
    "        ld11 = nn.FullConnectLayer(ld1, 60, d.linear, rng)\n",
    "        ld22 = nn.FullConnectLayer(ld2, 60, d.linear, rng)\n",
    "        ld33 = nn.FullConnectLayer(ld3, 60, d.linear, rng)\n",
    "\n",
    "        l = nn.MergeLayer([ld11, ld22, ld33])\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for (n, activation) in zip(layer_sizes, activations):\n",
    "        l = nn.FullConnectLayer(l, n, activation, rng)\n",
    "    return nn.NeuralNetwork(l, loss)\n",
    "\n",
    "\n",
    "def get_nn4(layer_sizes, activations, loss1, loss2):\n",
    "    assert len(layer_sizes) == len(activations)\n",
    "    \n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    def get_day_layer(num):\n",
    "        l = nn.InputLayer(120, f\"d{num}\")\n",
    "        return nn.FullConnectLayer(l, 60, d.linear, rng)\n",
    "\n",
    "    def get_days_layer():\n",
    "        ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n",
    "        l = nn.MergeLayer(ls)\n",
    "        return nn.FullConnectLayer(l, 100, d.linear, rng)\n",
    "\n",
    "    def get_city_layer():\n",
    "        coh = nn.InputLayer(36, \"city_one_hot\")\n",
    "        date = nn.InputLayer(1, \"date\")\n",
    "        coords = nn.InputLayer(2, \"coords\")\n",
    "        l = nn.MergeLayer([coh, date, coords])\n",
    "        return nn.FullConnectLayer(l, 20, d.linear, rng)\n",
    "\n",
    "    ds = get_days_layer()\n",
    "    c = get_city_layer()\n",
    "    l = nn.MergeLayer([ds, c])\n",
    "    for i in range(len(layer_sizes)):\n",
    "        if i == len(layer_sizes) - 2:\n",
    "            break\n",
    "        l = nn.FullConnectLayer(l, layer_sizes[i], activations[i], rng)\n",
    "    l_temp = nn.FullConnectLayer(l, layer_sizes[-2], activations[-2], rng)\n",
    "    l_wind = nn.FullConnectLayer(l, layer_sizes[-1], activations[-1], rng)\n",
    "    return (nn.NeuralNetwork(l_temp, loss1), nn.NeuralNetwork(l_wind, loss2))\n",
    "\n",
    "\n",
    "def get_nn_mid_prediction(loss):\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 50, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 1, d.linear, rng)\n",
    "\n",
    "    \n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d2_layer, d3_layer, d4_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    output_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 300, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 140, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 50, d.linear, rng)\n",
    "    output_layer = nn.FullConnectLayer(output_layer, 1, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(output_layer, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.74%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~15.77% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 15)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l2_loss)  # ~15.91%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.13%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~16.29% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~18.17%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net4_temp, net4_wind = get_nn4([300, 100, 60, 2, 1, 2], [d.relu, d.relu, d.relu, d.relu, d.linear, d.softmax], d.l2_loss, d.cross_entropy_loss)  # 15.66% + 61.49%\n",
    "net4_temp.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "net4_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # no shared weights\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 61.49% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "predicted = net3_wind.predict(train_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = train_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net3_wind.predict(test_set)\n",
    "print(predicted)\n",
    "print(np.max(predicted[0, :]), np.min(predicted[1, :]))\n",
    "predicted = np.rint(predicted[0, :])\n",
    "expected = test_set[\"output_wind\"][0, :]\n",
    "print(predicted)\n",
    "print(expected)\n",
    "print(np.count_nonzero(predicted == 1))\n",
    "print(predicted.size)\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(predicted == expected)}, bad predictions: {np.count_nonzero(predicted != expected)}, success_rate: {np.count_nonzero(predicted == expected) / predicted.size * 100 : .2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, aggregation, 1 prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best ones yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = get_nn_merge_after_while([100, 40, 1], [d.linear, d.linear, d.linear], d.l2_loss)  # ~12.77%\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3 = get_nn3([300, 100, 60, 1, 1], [d.relu, d.relu, d.relu, d.relu, d.linear], d.l2_loss)  # ~17.61% shared weights for days\n",
    "net3.train(train_set, test_set, 1, \"output_temp\", rng, 2)\n",
    "\n",
    "net3 = get_nn_merge_initially([300, 100, 60, 1, 1], [d.sigmoid, d.sigmoid, d.sigmoid, d.sigmoid, d.linear], d.l1_loss)  # ~17.51% shared weights for days\n",
    "net3.train(train_set, test_set, 1024, \"output_temp\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)\n",
    "\n",
    "net3_wind = get_nn_only_days([80, 40, 2, 2, 2], [d.linear, d.linear, d.linear, d.sigmoid, d.softmax], d.cross_entropy_loss)  # 53.18% no matter hinge or cross entropy shared weights for days\n",
    "net3_wind.train(train_set, test_set, 1024, \"output_wind\", rng, 5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No neighbors, no aggregation, 24 predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_24_predictions():\n",
    "    rng = np.random.default_rng(1)\n",
    "\n",
    "    d1_layer = nn.InputLayer(120, \"d1\")\n",
    "    d2_layer = nn.InputLayer(120, \"d2\")\n",
    "    d3_layer = nn.InputLayer(120, \"d3\")\n",
    "    days_layer = nn.MergeLayer([d1_layer, d2_layer, d3_layer])\n",
    "\n",
    "    coh_layer = nn.InputLayer(36, \"city_one_hot\")\n",
    "    date_layer = nn.InputLayer(1, \"date\")\n",
    "    coords_layer = nn.InputLayer(2, \"coords\")\n",
    "    city_layer = nn.MergeLayer([coh_layer, date_layer, coords_layer])\n",
    "\n",
    "    d4_layer = nn.MergeLayer([days_layer, city_layer])\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 300, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 140, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 60, d.linear, rng)\n",
    "    d4_layer = nn.FullConnectLayer(d4_layer, 24, d.linear, rng)\n",
    "\n",
    "    return nn.NeuralNetwork(d4_layer, d.l2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_24pred = get_nn_24_predictions()\n",
    "net_24pred.train(train_set, test_set, 512, \"output_temp\", rng, 10)  # only works without batching in nn (too big dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = net_24pred.predict(train_set)\n",
    "expected = train_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[train] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[train] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")\n",
    "\n",
    "predicted = net_24pred.predict(test_set)\n",
    "expected = test_set[\"output_temp\"]\n",
    "\n",
    "predicted = denormalized(predicted, params[\"temperature\"])\n",
    "expected = denormalized(expected, params[\"temperature\"])\n",
    "\n",
    "predicted = np.mean(predicted, axis=0)\n",
    "expected = np.mean(expected, axis=0)\n",
    "\n",
    "print(predicted)\n",
    "print(expected)\n",
    "\n",
    "diffs = np.abs(predicted - expected)\n",
    "print(f\"[test] min: {np.min(diffs)}, max: {np.max(diffs)}, mean: {np.mean(diffs)}, median: {np.median(diffs)}\")\n",
    "print(f\"[test] Good predictions: {np.count_nonzero(diffs <= 2)}, bad predictions: {np.count_nonzero(diffs > 2)}, success rate: {np.count_nonzero(diffs <= 2) / diffs.size * 100 : .2f}%\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT",
     "timestamp": 1666562444086
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
