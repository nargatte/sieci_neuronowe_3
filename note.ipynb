{"cells":[{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["import datetime\n","import definitions as d\n","import geopy.distance\n","import neural_network as nn\n","import numpy as np\n","import os\n","import pandas as pd\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","\n","def load_raw_data(path, type):\n","    files = os.listdir(f\"{path}/{type}\")\n","    data = {}\n","    surfix = f\"_{type}.csv\"\n","    for file in files:\n","        name = file[:file.find(surfix)]\n","        df = pd.read_csv(f\"{path}/{type}/{file}\", sep=\";\")\n","        data[name] = df\n","    return data\n","\n","def get_nearest_cities(city_attributes):\n","    nearest_cities = {}\n","    for _, row in city_attributes.iterrows():\n","        source = (row[\"Latitude\"], row[\"Longitude\"])\n","        city_dist = []\n","        for _, row2 in city_attributes.iterrows():\n","            if row[\"City\"] is row2[\"City\"]:\n","                continue\n","            destination = (row2[\"Latitude\"], row2[\"Longitude\"])\n","            city_dist.append((row2[\"City\"], geopy.distance.geodesic(source, destination).km))\n","        city_dist.sort(key=lambda x: x[1])\n","        nearest_cities[row[\"City\"]] = [cd[0] for cd in city_dist[:3]]\n","    return nearest_cities\n","    \n","def load_train():\n","    dict = load_raw_data(\"data\", \"train\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[12:, :]\n","    return dict\n","\n","def load_test():\n","    dict = load_raw_data(\"data\", \"test\")\n","    dict.pop(\"weather_description\")\n","    for key, df in dict.items():\n","        dict[key] = df.iloc[:-1, :]\n","    return dict\n","\n","def get_normalization_params(raw):\n","    params = {}\n","    for key, df in raw.items():\n","        all = np.reshape(df.to_numpy()[:, 1:], -1)\n","        params[key] = (np.nanmean(all), np.nanstd(all))\n","    return params\n","\n","def to_city_time_vect(raw):\n","    cities = next(iter(raw.values())).columns[1:]\n","    hours = next(iter(raw.values()))[[\"datetime\"]]\n","    ctvs = {c: hours.copy() for c in cities}\n","    for city in cities:\n","        for key, df in raw.items():\n","            ctvs[city][key] = df[[city]]\n","    return ctvs\n","\n","def normalize(ctv, params):\n","    for df in ctv.values():\n","        for param, ms in params.items():\n","            mean, std = ms\n","            df[param] = (df[param] - mean) / std \n","\n","def normalize_city_attributes(city_attributes):\n","    latitude_mean = city_attributes[\"Latitude\"].mean()\n","    latitude_std = city_attributes[\"Latitude\"].std()\n","\n","    longitude_mean = city_attributes[\"Longitude\"].mean()\n","    longitude_std = city_attributes[\"Longitude\"].std()\n","\n","    city_attributes[\"Latitude\"] = (city_attributes[\"Latitude\"] - latitude_mean) / latitude_std\n","    city_attributes[\"Longitude\"] = (city_attributes[\"Longitude\"] - longitude_mean) / longitude_std\n","\n","def to_city_day_vect(ctv, wind_treshold):\n","    cdv = {}\n","    for city, df in ctv.items():\n","        u = 0\n","        cdr = []\n","        while u < len(df):\n","            w = u + 24\n","            date = df.iloc[u, 0]\n","            vec = np.reshape(df.iloc[u : w, 1:].to_numpy(), -1)\n","            temp_mean = df.iloc[u : w][\"temperature\"].mean()\n","            wind_cat = int(np.any(df.iloc[u:w][\"wind_speed\"].to_numpy() > wind_treshold))\n","            cdr.append((date, vec, temp_mean, wind_cat))\n","            u = w\n","        cdv[city] = cdr\n","    return cdv\n","\n","def get_city_encoder(cities_attr):\n","    cities = np.reshape(city_attributes_raw[\"City\"].to_numpy(), (-1, 1))\n","    cohe = OneHotEncoder()\n","    cohe.fit(cities)\n","    return cohe\n","\n","def get_wind_treshold(souce, params):\n","    mean, std = params[\"wind_speed\"]\n","    return (souce - mean) / std\n","\n","def drop_nan_records(data_set):\n","    mask = [np.any(np.isnan(val), axis=0) for val in data_set.values()]\n","    mask = np.vstack(mask)\n","    mask = np.any(mask, axis=0)\n","    return {key: val[:, ~mask] for key, val in data_set.items()}\n","\n","def get_set1(cdv, city_encoder, city_attributes_raw):\n","    d1 = []\n","    d2 = []\n","    d3 = []\n","    output_temp = []\n","    output_wind = []\n","    date = []\n","    city_one_hot = []\n","    cord = []\n","\n","    for city, dv in cdv.items():\n","        d1 += [r[1] for r in dv[:-4]]\n","        d2 += [r[1] for r in dv[1:-3]]\n","        d3 += [r[1] for r in dv[2:-2]]\n","        output_temp += [r[2] for r in dv[4:]]\n","        output_wind += [r[3] for r in dv[4:]]\n","        date_str = [r[0] for r in dv[4:]]\n","        date += [datetime.datetime.strptime(d, \"%d.%m.%Y %H:%M\").timetuple().tm_yday / 365 - 0.5 for d in date_str]\n","        size = len(date_str)\n","        city_one_hot += [city_encoder.transform([[city]]).toarray()[0]] * size\n","        cord += [city_attributes_raw.loc[city_attributes_raw[\"City\"] == city][[\"Latitude\", \"Longitude\"]].to_numpy()] * size\n","\n","    set = {\n","        \"d1\": d1,\n","        \"d2\": d2,\n","        \"d3\": d3,\n","        \"output_temp\": output_temp,\n","        \"output_wind\": output_wind,\n","        \"date\": date,\n","        \"city_one_hot\": city_one_hot,\n","        \"cord\": cord\n","    }\n","\n","    return {key: np.vstack(val).T for key, val in set.items()}\n","\n","city_attributes_raw = pd.read_csv(\"data/city_attributes.csv\", sep=\";\")\n","\n","train_raw = load_train()\n","nearest_cities = get_nearest_cities(city_attributes_raw)\n","normalization_params = get_normalization_params(train_raw)\n","train_ctv = to_city_time_vect(train_raw)\n","normalize(train_ctv, normalization_params)\n","normalize_city_attributes(city_attributes_raw)\n","wind_treshold = get_wind_treshold(6, normalization_params)\n","train_cdv = to_city_day_vect(train_ctv, wind_treshold)\n","city_encoder = get_city_encoder(city_attributes_raw)\n","train_set = get_set1(train_cdv, city_encoder, city_attributes_raw)\n","# train_set = drop_nan_records(train_set)\n","\n","test_raw = load_test()\n","test_ctv = to_city_time_vect(test_raw)\n","normalize(test_ctv, normalization_params)\n","test_cdv = to_city_day_vect(test_ctv, wind_treshold)\n","test_set = get_set1(test_cdv, city_encoder, city_attributes_raw)\n","# test_set = drop_nan_records(test_set)"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["import neural_network as nn\n","import definitions as d\n","\n","rng = np.random.default_rng(0)\n","\n","def get_day_layer(num):\n","    l = nn.InputLayer(120, f\"d{num}\")\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_days_layer():\n","    ls = [get_day_layer(1), get_day_layer(2), get_day_layer(3)]\n","    l = nn.MergeLayer(ls)\n","    return nn.FullConnectLayer(l, 60, d.relu, rng)\n","\n","def get_city_layer():\n","    coh = nn.InputLayer(36, \"city_one_hot\")\n","    date = nn.InputLayer(1, \"date\")\n","    cord = nn.InputLayer(2, \"cord\")\n","    return nn.MergeLayer([coh, date, cord])\n","\n","def get_nn(layer_sizes, activations, loss):\n","    assert len(layer_sizes) == len(activations)\n","\n","    ds = get_days_layer()\n","    c = get_city_layer()\n","    l = nn.MergeLayer([ds, c])\n","    for (n, activation) in zip(layer_sizes, activations):\n","        l = nn.FullConnectLayer(l, n, activation, rng)\n","    return nn.NeuralNetwork(l, loss)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net2 \u001b[39m=\u001b[39m get_nn([\u001b[39m60\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m2\u001b[39m], [d\u001b[39m.\u001b[39mrelu, d\u001b[39m.\u001b[39msigmoid, d\u001b[39m.\u001b[39msoftmax], d\u001b[39m.\u001b[39mcross_entropy_loss)\n\u001b[0;32m----> 2\u001b[0m net2\u001b[39m.\u001b[39;49mtrain(train_set, test_set, \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_wind\u001b[39;49m\u001b[39m\"\u001b[39;49m, rng)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:194\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, train_set, test_set, batch_size, output_name, rng)\u001b[0m\n\u001b[1;32m    192\u001b[0m     expected \u001b[39m=\u001b[39m batch[output_name]\n\u001b[1;32m    193\u001b[0m     losses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(predicted, expected))\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward(expected)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights()\n\u001b[1;32m    196\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(losses)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:143\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward\u001b[0;34m(self, expected)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_backward\u001b[39m(\u001b[39mself\u001b[39m, expected):\n\u001b[1;32m    142\u001b[0m     da \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss[\u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma, expected)\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer, da)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:154\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(mi\u001b[39m.\u001b[39minput_layer, nda)\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mpropagate_backward(da)\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(layer\u001b[39m.\u001b[39minput_layer, nda)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:60\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_backward\u001b[0;34m(self, da)\u001b[0m\n\u001b[1;32m     57\u001b[0m         dz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice) \u001b[39mif\u001b[39;00m dz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mhstack((dz, np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice)))\n\u001b[1;32m     59\u001b[0m dW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mT) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39msum(dz, \u001b[39m1\u001b[39m), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mupdate_derivative(dW, db)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mT, dz)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:60\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_backward\u001b[0;34m(self, da)\u001b[0m\n\u001b[1;32m     57\u001b[0m         dz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice) \u001b[39mif\u001b[39;00m dz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mhstack((dz, np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice)))\n\u001b[1;32m     59\u001b[0m dW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(dz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mx\u001b[39m.\u001b[39mT) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 60\u001b[0m db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39msum(dz, \u001b[39m1\u001b[39m), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     61\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mupdate_derivative(dW, db)\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mW\u001b[39m.\u001b[39mT, dz)\n","File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1395\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1344\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[39m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_threads_suspended_single_notification\u001b[39m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_wait_suspend(thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[39mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[39m# This means that we should pause again after a set next statement.\u001b[39;00m\n","File \u001b[0;32m~/.local/lib/python3.10/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[1;32m   2108\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[39mstr\u001b[39m(\u001b[39mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[39m# process any stepping instructions\u001b[39;00m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["net2 = get_nn([60, 20, 2], [d.relu, d.sigmoid, d.softmax], d.cross_entropy_loss)\n","net2.train(train_set, test_set, 1024, \"output_wind\", rng)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Train: 0.23989644624107975, test: 1.549964456313118\n","Train: 0.11328532739660888, test: 1.6732014095030603\n","Train: 0.10447389469839531, test: 1.7348990708800631\n","Train: 0.09915099681070151, test: 1.7099880592021324\n","Train: 0.0953802111296878, test: 1.6726657026078102\n","Train: 0.09187527024395371, test: 1.636069154311085\n","Train: 0.08869845809289927, test: 1.57824380989809\n","Train: 0.08590923306149792, test: 1.5451232939103718\n","Train: 0.08380313094703273, test: 1.5019675628669582\n","Train: 0.08273224193283998, test: 1.4599433875430916\n","Train: 0.08091403524347734, test: 1.4703741700289505\n","Train: 0.07984315748641627, test: 1.496271700265284\n","Train: 0.07768734791819104, test: 1.45642805118814\n","Train: 0.0766122089205214, test: 1.4766793432173422\n","Train: 0.07594758653617394, test: 1.418715028507999\n","Train: 0.0748376553635245, test: 1.4305429581089846\n","Train: 0.07374439199246278, test: 1.3987005072175998\n","Train: 0.07316565872409382, test: 1.3659230405843878\n","Train: 0.07208331507284216, test: 1.397997137563094\n","Train: 0.0711415200456233, test: 1.2853387700186598\n","Train: 0.07127427304272367, test: 1.3286943432861464\n","Train: 0.07151315235529135, test: 1.3532413251478974\n","Train: 0.07322582858880845, test: 1.320805150647228\n","Train: 0.07440497019032731, test: 1.3486342664290132\n","Train: 0.07325213373768749, test: 1.3805742259691205\n","Train: 0.07289898464882603, test: 1.295445190584709\n","Train: 0.07277486677570716, test: 1.2521598776491951\n","Train: 0.07395592116718323, test: 1.3732739242413015\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m net \u001b[39m=\u001b[39m get_nn([\u001b[39m60\u001b[39m, \u001b[39m1\u001b[39m], [d\u001b[39m.\u001b[39mrelu, d\u001b[39m.\u001b[39mlinear], d\u001b[39m.\u001b[39ml2_loss)\n\u001b[0;32m----> 2\u001b[0m net\u001b[39m.\u001b[39;49mtrain(train_set, test_set, \u001b[39m1024\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39moutput_temp\u001b[39;49m\u001b[39m\"\u001b[39;49m, rng)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:194\u001b[0m, in \u001b[0;36mNeuralNetwork.train\u001b[0;34m(self, train_set, test_set, batch_size, output_name, rng)\u001b[0m\n\u001b[1;32m    192\u001b[0m     expected \u001b[39m=\u001b[39m batch[output_name]\n\u001b[1;32m    193\u001b[0m     losses\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcalculate_loss(predicted, expected))\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward(expected)\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_weights()\n\u001b[1;32m    196\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(losses)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(losses)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:143\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward\u001b[0;34m(self, expected)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpropagate_backward\u001b[39m(\u001b[39mself\u001b[39m, expected):\n\u001b[1;32m    142\u001b[0m     da \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss[\u001b[39m\"\u001b[39m\u001b[39md\u001b[39m\u001b[39m\"\u001b[39m](\u001b[39mself\u001b[39m\u001b[39m.\u001b[39ma, expected)\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput_layer, da)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:155\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mpropagate_backward(da)\n\u001b[0;32m--> 155\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate_backward_req(layer\u001b[39m.\u001b[39;49minput_layer, nda)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:154\u001b[0m, in \u001b[0;36mNeuralNetwork.propagate_backward_req\u001b[0;34m(self, layer, da)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(mi\u001b[39m.\u001b[39minput_layer, nda)\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     nda \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mpropagate_backward(da)\n\u001b[1;32m    155\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpropagate_backward_req(layer\u001b[39m.\u001b[39minput_layer, nda)\n","File \u001b[0;32m~/projects/sieci_neuronowe_3/neural_network.py:58\u001b[0m, in \u001b[0;36mFullConnectLayer.propagate_backward\u001b[0;34m(self, da)\u001b[0m\n\u001b[1;32m     54\u001b[0m         da_slice \u001b[39m=\u001b[39m da[:, j]\u001b[39m.\u001b[39mreshape((\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[1;32m     56\u001b[0m         dz \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice) \u001b[39mif\u001b[39;00m dz \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mhstack((dz, np\u001b[39m.\u001b[39mdot(da_dz_slice, da_slice)))\n\u001b[0;32m---> 58\u001b[0m dW \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(dz, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mx\u001b[39m.\u001b[39;49mT) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     59\u001b[0m db \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mreshape(np\u001b[39m.\u001b[39msum(dz, \u001b[39m1\u001b[39m), (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)) \u001b[39m/\u001b[39m da\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m     60\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights\u001b[39m.\u001b[39mupdate_derivative(dW, db)\n","File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["net = get_nn([60, 1], [d.relu, d.linear], d.l2_loss)\n","net.train(train_set, test_set, 1024, \"output_temp\", rng)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[{"file_id":"1L9cEEvqFY-LfGpRe5CmiFLJeTtYx6dUT","timestamp":1666562444086}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"vscode":{"interpreter":{"hash":"e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"}}},"nbformat":4,"nbformat_minor":0}
